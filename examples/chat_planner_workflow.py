#!/usr/bin/env python3
"""
Chat Planner Workflow Example.

This workflow takes a user prompt via chat, uses a planning task
to generate an execution plan, dynamically generates tasks based on the plan,
executes them, and returns the result.
"""  # noqa: D202

import sys
import os
import logging
import json # Added for potential parsing
import re # Needed for JSON cleaning
import jsonschema # <-- Add import for JSON schema validation
from typing import Dict, Any

# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from core.workflow import Workflow
from core.task import Task, DirectHandlerTask
# Assuming Agent/Engine will be used to run this
# from core.agent import Agent
# from core.engine import WorkflowEngine
from core.services import get_services, reset_services
from core.llm.interface import LLMInterface
from core.tools.registry_access import execute_tool, tool_exists, register_tool
from core.handlers.registry_access import get_handler, handler_exists, register_handler
from core.tools.registry import ToolRegistry
from core.handlers.registry import HandlerRegistry
from core.tools.framework_tools import get_available_capabilities
# Import the correct visualizer function
from core.utils.visualizer import visualize_workflow # Correct function name


# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("chat_planner_workflow")

# --- Define JSON Schema for the Plan ---
# Describes the expected structure of the JSON list generated by the LLM planner.
PLAN_SCHEMA = {
    "type": "array",
    "items": {
        "type": "object",
        "properties": {
            "step_id": {"type": "string", "minLength": 1, "description": "Unique identifier for the step."},
            "description": {"type": "string", "description": "Natural language description of the step's purpose."},
            "type": {"type": "string", "enum": ["tool", "handler"], "description": "The type of capability to execute."},
            "name": {"type": "string", "minLength": 1, "description": "The exact name of the tool or handler."},
            "inputs": {
                "type": "object",
                "description": "Key-value pairs for inputs. Values can be literals or variable references like ${...}.",
                "additionalProperties": True # Allow any properties
            },
            "outputs": {
                "type": "array",
                "items": {"type": "string"},
                "description": "(Optional) List of expected output keys from this step."
            },
            "depends_on": {
                "type": "array",
                "items": {"type": "string"},
                "description": "List of step_ids this step directly depends on. Empty for the first step."
            }
        },
        "required": ["step_id", "description", "type", "name", "inputs", "depends_on"],
        "additionalProperties": False # Disallow extra properties at the step level
    },
    "description": "A list of steps defining the execution plan."
}

# --- Handler Implementations ---

def plan_user_request_handler(task: DirectHandlerTask, input_data: dict) -> dict:
    """
    Handler for the "Think & Analyze" planning task.
    Takes user request and context, generates a structured plan using an LLM.
    """
    logger.info(f"Executing planning handler for task: {task.id}")
    user_request = input_data.get("user_request", "")
    available_tools_str = input_data.get("available_tools_context", "No tools provided.")
    available_handlers_str = input_data.get("available_handlers_context", "No handlers provided.")

    if not user_request:
        logger.error("No user_request provided to planning handler.")
        return {"success": False, "error": "Missing user_request input."}

    # Refined LLM Prompt for Planning
    prompt = f"""
    You are an expert AI planning assistant for the Dawn workflow framework.
    Your objective is to analyze a user's request and the available capabilities (tools and handlers)
    to generate a precise, structured, and executable plan as a JSON list of steps.

    **User Request:**
    ```
    {user_request}
    ```

    **Available Tools (Use ONLY these):**
    ```
    {available_tools_str}
    ```

    **Available Handlers (Use ONLY these):**
    ```
    {available_handlers_str}
    ```

    **Instructions for Generating the Plan:**

    1.  **Decompose:** Break down the user request into a sequence of logical, distinct steps required to achieve the goal.
    2.  **Map to Capabilities:** For each step, identify the *most appropriate* available tool or handler from the lists provided above. Match the step's purpose to the tool/handler description. If no suitable capability exists, you cannot create a step for it.
    3.  **Define Inputs:** For each step, determine the necessary inputs for the chosen tool/handler. Map these inputs using the following variable notation:
        *   Use `${{user_prompt}}` to refer to the original user request text provided above.
        *   Use `${{step_id.output.field_name}}` to refer to an output field from a *previous* step (replace `step_id` and `field_name` accordingly). Assume standard outputs might be under `.result` or `.response`. Specify the *exact* expected path.
        *   For fixed values, use the actual value (e.g., `{{"file_path": "/data/report.pdf"}}`).
        *   Ensure all required inputs for the selected tool/handler are specified.
    4.  **Define Dependencies:** For each step, list the `step_id`s of all *direct* prerequisite steps in the `depends_on` list. The first step should have an empty list `[]`.
    5.  **Structure Output:** Format the entire plan as a single JSON list `[...]`. Each element in the list must be a JSON object representing one step, adhering strictly to the following schema:
        ```json
        {{
          "step_id": "string", // Unique identifier (e.g., "step_1", "step_2")
          "description": "string", // Clear natural language description of this step's goal
          "type": "string", // Must be exactly "tool" or "handler"
          "name": "string", // The exact name of the chosen tool or handler from the lists above
          "inputs": {{}}, // JSON object mapping input parameter names to values or variable references (e.g., {{"query": "${{user_prompt}}", "context": "${{step_1.output.result}}"}} )
          "outputs": ["string"], // Optional: List of key output names expected from this step (e.g., ["summary_text", "entity_list"])
          "depends_on": ["string"] // List of step_ids this step directly depends on (empty for the first step)
        }}
        ```
    6.  **Constraints:**
        *   Only use tools and handlers listed in the provided context. Do not invent capabilities.
        *   Ensure the plan is logical and dependencies are correct.
        *   The `inputs` for a step must only reference outputs from steps listed in its `depends_on` array or the initial `user_prompt`.

    **CRITICAL:** Your response **MUST** be **ONLY** the valid JSON list representing the plan. Do **NOT** include any introductory text, explanations, markdown formatting (like ```json), or concluding remarks. The entire response must start with `[` and end with `]`.
    """
    # Ensure the entire formatted string, including newlines, is within triple quotes
    logger.debug(f"""Planning prompt constructed (first 500 chars):
{prompt[:500]}...""")

    # TODO: 2. Call the LLM service
    #    - Need access to the LLMInterface from services.
    try:
        services = get_services()
        # Use the getter method to retrieve the LLM interface
        llm_interface = services.get_llm_interface() # Correct way to get LLM interface
        if not llm_interface:
             raise ValueError("LLMInterface not found in services.")

        # Note: Using a synchronous call here for simplicity in the handler.
        # If used in async engine, consider making handler async or using asyncio.to_thread
        logger.info("Calling LLM for planning...")
        llm_response = llm_interface.execute_llm_call(prompt) # Basic call, may need more params
        logger.info("LLM call completed.")

        if not llm_response.get("success"):
            error_msg = llm_response.get("error", "Unknown LLM error")
            logger.error(f"LLM call failed: {error_msg}")
            return {"success": False, "error": f"LLM planning failed: {error_msg}"}

        raw_plan_output = llm_response.get("response", "")
        if not raw_plan_output:
            logger.error("LLM returned empty response.")
            return {"success": False, "error": "LLM returned empty planning response."}

        # Log using separate arguments to avoid f-string interpolation issues
        logger.debug("Raw LLM plan output: %s", raw_plan_output)

        # TODO: 3. Parse and Validate the LLM's plan output
        #    - Ensure it's valid JSON.
        #    - Validate against the required plan schema (list of steps, required fields per step).
        #    - Handle potential JSON parsing errors or schema validation errors.
        try:
            # Basic parsing attempt
            plan = json.loads(raw_plan_output)
            if not isinstance(plan, list):
                 raise ValueError("Plan is not a JSON list.")
            # TODO: Add more detailed schema validation here
            logger.info(f"Successfully parsed plan with {len(plan)} steps.")

            # Return the structured plan AND the raw output for validation
            return {
                "success": True,
                "result": {
                    "plan": plan, # The structured plan (might be empty if parsing failed)
                    "raw_llm_output": raw_plan_output # Pass raw output for validation task
                }
            }
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"Failed to parse/validate LLM plan output: {e}")
            logger.error(f"Raw output was: {raw_plan_output}")
            # Still return success=True but indicate parsing failure in result
            # The validation step will handle this specific case
            return {
                 "success": True, # Handler itself succeeded, but parsing failed
                 "result": {
                    "plan": None,
                    "raw_llm_output": raw_plan_output,
                    "parsing_error": str(e)
                 },
                 "error": f"Failed to parse/validate plan JSON: {e}" # Keep error for logging
             }

    except Exception as e:
        logger.exception(f"Error during planning handler execution: {e}")
        return {"success": False, "error": f"Internal planning handler error: {str(e)}"}


def validate_plan_handler(task: DirectHandlerTask, input_data: dict) -> dict:
    """
    Handler to validate the structure and content of a plan generated by the LLM.
    Checks JSON validity, schema compliance, existence of specified tools/handlers,
    and basic dependency logic.
    """
    logger.info(f"Executing plan validation handler for task: {task.id}")
    raw_plan_output = input_data.get("raw_llm_output")
    tool_details = input_data.get("tool_details", [])
    handler_details = input_data.get("handler_details", [])

    if raw_plan_output is None: # Check for None specifically
        logger.error("Missing 'raw_llm_output' in input for validation.")
        return {"success": False, "error": "Missing raw plan output for validation."}
    if not tool_details and not handler_details:
         logger.warning("No tool or handler details provided for validation. Skipping capability checks.")
         # Allow validation to proceed but capability checks will be skipped

    available_tool_names = {t['name'] for t in tool_details}
    available_handler_names = {h['name'] for h in handler_details}
    validation_errors = []
    validation_warnings = []
    parsed_plan = None

    # Summary variables to help generate useful error messages
    error_summary = {
        "has_json_error": False,
        "has_schema_error": False,
        "has_tool_handler_error": False,
        "has_dependency_error": False,
        "has_input_reference_error": False,
        "error_count": 0,
        "most_critical_error": None,
        "example_errors": []
    }

    try:
        # 1. Clean potential markdown and parse JSON
        cleaned_json_string = re.sub(r"^```json\s*|\s*```$", "", raw_plan_output, flags=re.MULTILINE).strip()
        if not cleaned_json_string:
            error_summary["has_json_error"] = True
            error_summary["most_critical_error"] = "Empty plan output"
            error_summary["error_count"] += 1
            error_message = "Cleaned plan output is empty."
            validation_errors.append(error_message)
            error_summary["example_errors"].append(error_message)
            raise ValueError(error_message)

        try:
            parsed_plan = json.loads(cleaned_json_string)
        except json.JSONDecodeError as json_err:
            error_summary["has_json_error"] = True
            error_summary["most_critical_error"] = f"JSON parsing error: {json_err}"
            error_summary["error_count"] += 1
            error_message = f"Failed to parse JSON: {json_err}"
            validation_errors.append(error_message)
            error_summary["example_errors"].append(error_message)
            
            # Attempt to recover malformed JSON through common fixes
            recovered_plan = attempt_json_recovery(cleaned_json_string)
            if recovered_plan is not None:
                logger.info("Successfully recovered from malformed JSON")
                parsed_plan = recovered_plan
                validation_warnings.append("Used JSON recovery to parse the plan. The plan may not be exactly as intended.")
            else:
                raise ValueError("Could not recover from malformed JSON")

        # 2. Validate against JSON Schema
        if parsed_plan is not None:
            try:
                jsonschema.validate(instance=parsed_plan, schema=PLAN_SCHEMA)
                logger.info("Plan successfully validated against JSON schema.")
            except jsonschema.exceptions.ValidationError as schema_err:
                error_summary["has_schema_error"] = True
                if not error_summary["most_critical_error"]:
                    error_summary["most_critical_error"] = f"Schema validation error: {schema_err.message}"
                error_summary["error_count"] += 1
                
                # Create a more user-friendly error message
                error_path = "/".join(map(str, schema_err.path)) if schema_err.path else "root"
                error_message = f"JSON Schema validation failed at {error_path}: {schema_err.message}"
                validation_errors.append(error_message)
                error_summary["example_errors"].append(error_message)
                
                # Continue with validation to find more errors
                validation_warnings.append("Proceeding with additional validation despite schema errors.")
                logger.warning(f"Schema validation failed: {schema_err.message}")

        # 3. Additional Semantic Checks (whether plan passed schema or not)
        # If the plan is not a list or is None, create a sensible fallback for further checks
        if not isinstance(parsed_plan, list):
            error_summary["has_schema_error"] = True
            if not error_summary["most_critical_error"]:
                error_summary["most_critical_error"] = "Plan is not a JSON list"
            error_summary["error_count"] += 1
            error_message = "Plan is not a JSON list. Expected an array of step objects."
            validation_errors.append(error_message)
            error_summary["example_errors"].append(error_message)
            
            # If we have any object that looks like a step, try to treat it as a single-item list
            if isinstance(parsed_plan, dict) and "step_id" in parsed_plan:
                validation_warnings.append("Treating single step object as a list for further validation.")
                parsed_plan = [parsed_plan]
            else:
                parsed_plan = []  # Empty list as fallback

        if not parsed_plan and not validation_errors:
             logger.info("Plan list is empty, but valid according to schema.")
             # Empty plans are technically valid (though not useful)
             validation_warnings.append("Plan is empty (contains no steps). This is valid but not useful.")

        # Collect step IDs first for dependency checks
        all_step_ids = set()
        step_ids_with_errors = set()
        for i, step in enumerate(parsed_plan):
             if not isinstance(step, dict): continue # Should be caught by schema
             step_id = step.get("step_id")
             if not step_id or not isinstance(step_id, str): # Should be caught by schema
                 step_ids_with_errors.add(f"Step {i+1} (No ID)")
                 error_summary["has_schema_error"] = True
                 error_summary["error_count"] += 1
                 continue
             if step_id in all_step_ids:
                 error_message = f"Step {i+1}: Duplicate 'step_id' found: '{step_id}'."
                 validation_errors.append(error_message)
                 error_summary["has_dependency_error"] = True
                 error_summary["error_count"] += 1
                 if len(error_summary["example_errors"]) < 3:
                     error_summary["example_errors"].append(error_message)
                 step_ids_with_errors.add(step_id)
             else:
                 all_step_ids.add(step_id)

        # Validate each step in the list (focus on non-schema checks now)
        for i, step in enumerate(parsed_plan):
            step_number = i + 1
            if not isinstance(step, dict): continue # Already handled

            step_id = step.get("step_id")
            step_name = step.get("name")
            step_type = step.get("type")
            depends_on = step.get("depends_on", [])

            # Check if tool/handler exists (if details provided)
            if step_name and step_type: # Should exist due to schema
                 if step_type == "tool" and available_tool_names and step_name not in available_tool_names:
                     error_message = f"Step {step_number} (ID: {step_id}): Specified tool '{step_name}' is not in the list of available tools."
                     validation_errors.append(error_message)
                     error_summary["has_tool_handler_error"] = True
                     error_summary["error_count"] += 1
                     if len(error_summary["example_errors"]) < 3:
                         error_summary["example_errors"].append(error_message)
                 elif step_type == "handler" and available_handler_names and step_name not in available_handler_names:
                     error_message = f"Step {step_number} (ID: {step_id}): Specified handler '{step_name}' is not in the list of available handlers."
                     validation_errors.append(error_message)
                     error_summary["has_tool_handler_error"] = True
                     error_summary["error_count"] += 1
                     if len(error_summary["example_errors"]) < 3:
                         error_summary["example_errors"].append(error_message)

            # Basic Dependency Check (ensure dependencies exist and are valid IDs)
            if step_id and step_id not in step_ids_with_errors and isinstance(depends_on, list):
                 for dep_id in depends_on:
                      if not isinstance(dep_id, str): # Should be caught by schema
                           error_message = f"Step {step_number} (ID: {step_id}): 'depends_on' contains non-string element '{dep_id}'."
                           validation_errors.append(error_message)
                           error_summary["has_dependency_error"] = True
                           error_summary["error_count"] += 1
                           if len(error_summary["example_errors"]) < 3:
                               error_summary["example_errors"].append(error_message)
                      elif dep_id not in all_step_ids:
                          error_message = f"Step {step_number} (ID: {step_id}): Dependency '{dep_id}' does not match any defined 'step_id'."
                          validation_errors.append(error_message)
                          error_summary["has_dependency_error"] = True
                          error_summary["error_count"] += 1
                          if len(error_summary["example_errors"]) < 3:
                              error_summary["example_errors"].append(error_message)
                      elif dep_id == step_id: # Check for self-dependency
                          error_message = f"Step {step_number} (ID: {step_id}): Step cannot depend on itself."
                          validation_errors.append(error_message)
                          error_summary["has_dependency_error"] = True
                          error_summary["error_count"] += 1
                          if len(error_summary["example_errors"]) < 3:
                              error_summary["example_errors"].append(error_message)

            # Check Input Variable References
            inputs = step.get("inputs", {})
            if isinstance(inputs, dict):
                # Pattern to match ${...} variable references
                var_pattern = re.compile(r'\${(.*?)}')
                
                for input_key, input_value in inputs.items():
                    if not isinstance(input_value, str):
                        continue  # Skip non-string values (they can't contain variable references)
                    
                    # Find all variable references in this input value
                    for match in var_pattern.finditer(input_value):
                        var_ref = match.group(1)  # Extract the variable reference without ${}
                        
                        # Case 1: Reference to the user prompt (always available)
                        if var_ref == "user_prompt":
                            continue  # This is valid - user_prompt is available to all steps
                        
                        # Case 2: Reference to output from another step
                        elif '.' in var_ref and var_ref.split('.')[1] == "output":
                            ref_parts = var_ref.split('.')
                            if len(ref_parts) < 3:
                                error_message = f"Step {step_number} (ID: {step_id}): Invalid output reference format in '{input_key}': '{var_ref}' - Expected format: 'step_id.output.field_path'"
                                validation_errors.append(error_message)
                                error_summary["has_input_reference_error"] = True
                                error_summary["error_count"] += 1
                                if len(error_summary["example_errors"]) < 3:
                                    error_summary["example_errors"].append(error_message)
                                continue
                                
                            source_step_id = ref_parts[0]
                            
                            # Check if referenced step exists
                            if source_step_id not in all_step_ids:
                                error_message = f"Step {step_number} (ID: {step_id}): Input '{input_key}' references non-existent step '{source_step_id}'"
                                validation_errors.append(error_message)
                                error_summary["has_input_reference_error"] = True
                                error_summary["error_count"] += 1
                                if len(error_summary["example_errors"]) < 3:
                                    error_summary["example_errors"].append(error_message)
                                continue
                                
                            # Check if referenced step is actually a dependency
                            if source_step_id not in depends_on:
                                error_message = f"Step {step_number} (ID: {step_id}): Input '{input_key}' references step '{source_step_id}' but it's not listed in 'depends_on'. Add it to ensure correct execution order."
                                validation_errors.append(error_message)
                                error_summary["has_input_reference_error"] = True
                                error_summary["error_count"] += 1
                                if len(error_summary["example_errors"]) < 3:
                                    error_summary["example_errors"].append(error_message)
                        
                        # Case 3: Some other variable format we don't recognize
                        else:
                            error_message = f"Step {step_number} (ID: {step_id}): Input '{input_key}' contains unrecognized variable reference format: '${{{var_ref}}}'. Only '{{user_prompt}}' and '{{step_id.output.field}}' are supported."
                            validation_errors.append(error_message)
                            error_summary["has_input_reference_error"] = True
                            error_summary["error_count"] += 1
                            if len(error_summary["example_errors"]) < 3:
                                error_summary["example_errors"].append(error_message)

        # Generate user-friendly error message
        if validation_errors:
            logger.warning(f"Plan validation failed with {len(validation_errors)} errors.")
            for error in validation_errors:
                logger.warning(f"- {error}")
                
            # Create a user-friendly formatted error message
            user_friendly_error = format_validation_errors_for_user(
                validation_errors, 
                error_summary, 
                raw_plan_output
            )
                
            # Return success=False but include the parsed plan and errors
            return {
                "success": False,
                "error": "Plan validation failed.",
                "validation_errors": validation_errors,
                "validation_warnings": validation_warnings,
                "error_summary": error_summary,
                "user_friendly_error": user_friendly_error,
                "parsed_plan": parsed_plan # Return the partially parsed plan for debugging
            }
        else:
            # Check if we have any warnings
            if validation_warnings:
                logger.info(f"Plan validation successful with {len(validation_warnings)} warnings.")
                for warning in validation_warnings:
                    logger.info(f"- {warning}")
            else:
                logger.info("Plan validation successful.")
                
            return {
                "success": True,
                "result": {
                    "validated_plan": parsed_plan
                },
                "validation_warnings": validation_warnings if validation_warnings else None
            }

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse plan JSON during validation: {e}")
        logger.error(f"Raw output was: {raw_plan_output}")
        
        # Generate user-friendly error with suggestions
        user_friendly_error = f"""
The plan couldn't be parsed as valid JSON. Error: {e}
        
This typically happens when:
1. The response contains non-JSON text before or after the JSON structure
2. There are syntax errors like missing commas, brackets, or quotes
3. The response format doesn't match the expected list of steps
        
Raw output: {raw_plan_output[:100]}...
"""
        
        return {
            "success": False, 
            "error": f"Validation failed: Invalid JSON - {e}", 
            "validation_errors": [f"Invalid JSON: {e}"],
            "user_friendly_error": user_friendly_error,
            "error_summary": {
                "has_json_error": True,
                "error_count": 1,
                "most_critical_error": f"JSON parsing error: {e}",
                "example_errors": [f"Invalid JSON: {e}"]
            }
        }
    except ValueError as e: # Catch structure errors like empty string
         logger.error(f"Plan data validation failed: {e}")
         
         user_friendly_error = f"""
The plan couldn't be validated due to a structural error: {e}
         
Please check if the plan contains all required information and follows the expected format.
"""
         
         return {
             "success": False, 
             "error": f"Validation failed: {e}", 
             "validation_errors": validation_errors or [str(e)],
             "user_friendly_error": user_friendly_error,
             "error_summary": error_summary or {
                 "has_schema_error": True,
                 "error_count": 1,
                 "most_critical_error": str(e),
                 "example_errors": [str(e)]
             }
         }
    except Exception as e:
        logger.exception(f"Unexpected error during plan validation: {e}")
        
        user_friendly_error = f"""
An unexpected error occurred during plan validation: {e}
        
This is likely a bug in the validation code rather than an issue with your plan.
Please report this issue with the following details:
- Error: {e}
- Error type: {type(e).__name__}
"""
        
        return {
            "success": False, 
            "error": f"Internal validation handler error: {str(e)}", 
            "validation_errors": [f"Internal error: {str(e)}"],
            "user_friendly_error": user_friendly_error,
            "error_summary": {
                "error_count": 1,
                "most_critical_error": f"Internal validation error: {e}",
                "example_errors": [f"Internal error: {str(e)}"]
            }
        }


def format_validation_errors_for_user(validation_errors, error_summary, raw_plan=None):
    """
    Creates a user-friendly error message summarizing validation errors.
    
    Args:
        validation_errors: List of detailed validation error messages
        error_summary: Dictionary with summary of error types and counts
        raw_plan: Optional raw plan string for reference
        
    Returns:
        A formatted string with user-friendly error explanation and suggestions
    """
    error_count = error_summary.get("error_count", len(validation_errors))
    
    # Start with a summary
    message = f"The plan has {error_count} validation error{'s' if error_count != 1 else ''}.\n\n"
    
    # Add the most critical error first
    if error_summary.get("most_critical_error"):
        message += f"Most critical issue: {error_summary['most_critical_error']}\n\n"
    
    # Categorize errors by type
    if error_summary.get("has_json_error"):
        message += "• JSON PARSING ERRORS: The plan couldn't be parsed as valid JSON. Check for syntax errors.\n"
    if error_summary.get("has_schema_error"):
        message += "• STRUCTURE ERRORS: The plan doesn't match the required schema (missing fields or wrong types).\n"
    if error_summary.get("has_tool_handler_error"):
        message += "• TOOL/HANDLER ERRORS: Some steps reference tools or handlers that don't exist.\n"
    if error_summary.get("has_dependency_error"):
        message += "• DEPENDENCY ERRORS: There are issues with step dependencies (missing references, cycles).\n"
    if error_summary.get("has_input_reference_error"):
        message += "• INPUT REFERENCE ERRORS: There are issues with variable references in step inputs.\n"
    
    # Add example errors (up to 3)
    if error_summary.get("example_errors"):
        message += "\nExample errors:\n"
        for i, error in enumerate(error_summary["example_errors"][:3]):
            message += f"{i+1}. {error}\n"
        
        if error_count > 3:
            message += f"...and {error_count - 3} more errors.\n"
    
    # Add suggestions
    message += "\nSuggestions:\n"
    
    if error_summary.get("has_json_error"):
        message += "• Make sure your plan is a valid JSON array of step objects\n"
        message += "• Check for missing commas, brackets, or quotes\n"
    
    if error_summary.get("has_schema_error"):
        message += "• Each step must have: step_id, description, type, name, inputs, depends_on\n"
        message += "• The 'type' field must be either 'tool' or 'handler'\n"
    
    if error_summary.get("has_tool_handler_error"):
        message += "• Only reference tools and handlers that exist in the system\n"
    
    if error_summary.get("has_dependency_error") or error_summary.get("has_input_reference_error"):
        message += "• Make sure all referenced step_ids exist in the plan\n"
        message += "• If a step uses output from another step, add that step to 'depends_on'\n"
        message += "• Use ${step_id.output.field} for referencing outputs from other steps\n"
    
    return message


def attempt_json_recovery(json_string):
    """
    Attempts to recover malformed JSON through common fixes.
    
    Args:
        json_string: The potentially malformed JSON string
        
    Returns:
        Parsed JSON object if recovery successful, None otherwise
    """
    # Try some common JSON recovery techniques
    try:
        # 1. Try with a different JSON parser that's more forgiving
        import json5
        try:
            return json5.loads(json_string)
        except:
            pass
    except ImportError:
        # json5 not available, try our manual fixes
        pass
    
    # 2. Try standard parser one more time
    try:
        return json.loads(json_string)
    except:
        pass
        
    # 3. Try fixing common issues
    fixes = [
        # Missing quotes around keys
        lambda s: re.sub(r'([{,]\s*)(\w+)(\s*:)', r'\1"\2"\3', s),
        
        # Single quotes instead of double
        lambda s: s.replace("'", '"'),
        
        # Trailing commas in arrays or objects
        lambda s: re.sub(r',\s*([}\]])', r'\1', s),
        
        # Missing closing brackets
        lambda s: s + '}' * (s.count('{') - s.count('}')),
        lambda s: s + ']' * (s.count('[') - s.count(']')),
        
        # Extract just the array portion if it's embedded in text
        lambda s: re.search(r'\[(.*)\]', s, re.DOTALL).group(0) if re.search(r'\[(.*)\]', s, re.DOTALL) else s,
    ]
    
    # Try each fix in sequence
    for fix_func in fixes:
        try:
            fixed = fix_func(json_string)
            result = json.loads(fixed)
            logger.info(f"JSON recovery successful using fix: {fix_func.__name__}")
            return result
        except:
            continue
            
    # If we get here, all recovery attempts failed
    return None


def plan_to_tasks_handler(task: DirectHandlerTask, input_data: dict) -> dict:
    """
    Converts a validated plan (list of steps) into a list of task definitions
    suitable for dynamic execution.
    """
    logger.info(f"Executing plan-to-tasks handler for task: {task.id}")
    validated_plan = input_data.get("plan") # Expecting the validated plan list

    if not isinstance(validated_plan, list):
        logger.error("Invalid input: 'plan' is missing or not a list.")
        return {"success": False, "error": "Validated plan is missing or not a list."}

    task_definitions = []
    conversion_errors = []

    for i, step in enumerate(validated_plan):
        step_number = i + 1
        try:
            logger.debug(f"Processing plan step {step_number}: {step.get('step_id', 'N/A')}")
            if not isinstance(step, dict):
                conversion_errors.append(f"Step {step_number}: Invalid format - not a dictionary.")
                continue

            # Extract core info, defaulting where necessary
            task_id = step.get("step_id")
            description = step.get("description", f"Generated Task {step_number}")
            step_type = step.get("type")
            name = step.get("name")
            inputs = step.get("inputs", {})
            depends_on = step.get("depends_on", []) # Capture dependencies

            if not task_id or not step_type or not name:
                 conversion_errors.append(f"Step {step_number}: Missing required field (step_id, type, or name).")
                 continue

            # Create the task definition dictionary
            task_def = {
                "task_id": task_id,
                "name": description, # Use plan description as task name
                "input_data": inputs,
                "depends_on": depends_on, # Store dependencies for potential later use by engine
                # Add other Task parameters if needed (e.g., max_retries)
                # "max_retries": step.get("retries", 0),
            }

            # Set type-specific parameters
            if step_type == "tool":
                task_def["is_llm_task"] = False # Assume tools are not LLM tasks unless specified otherwise
                task_def["tool_name"] = name
            elif step_type == "handler":
                 # For handlers, we'd ideally need a way to reference the handler function.
                 # If the engine supports creating DirectHandlerTasks from names,
                 # we might just pass the handler name.
                 # For now, let's assume the engine needs more info or handles this later.
                 # We will mark it distinctly.
                 task_def["is_llm_task"] = False
                 task_def["handler_name"] = name # Store handler name
                 # Indicate it's intended to be a DirectHandlerTask if engine supports it
                 task_def["task_class"] = "DirectHandlerTask"
            else:
                conversion_errors.append(f"Step {step_number} (ID: {task_id}): Invalid type '{step_type}'. Must be 'tool' or 'handler'.")
                continue

            task_definitions.append(task_def)
            logger.debug(f"Created task definition for step {task_id}.")

        except Exception as e:
            logger.exception(f"Error converting plan step {step_number} (ID: {step.get('step_id', 'N/A')}) to task definition: {e}")
            conversion_errors.append(f"Step {step_number} (ID: {step.get('step_id', 'N/A')}): Conversion error - {e}")

    if conversion_errors:
        logger.warning(f"Encountered {len(conversion_errors)} errors during plan-to-task conversion.")
        # Decide if partial success is allowed or if it should fail completely
        return {
            "success": False, # Fail if any conversion errors occurred
            "error": "Errors occurred during plan-to-task conversion.",
            "conversion_errors": conversion_errors,
            "partial_task_definitions": task_definitions # Include partially converted tasks for debugging
        }
    else:
        logger.info(f"Successfully converted plan into {len(task_definitions)} task definitions.")
        return {
            "success": True,
            "result": {
                "task_definitions": task_definitions
            }
        }


def execute_dynamic_tasks_handler(task: DirectHandlerTask, input_data: dict) -> dict:
    """
    Simulates the execution of dynamically generated task definitions.
    Executes tasks sequentially based on the provided list.
    Handles basic variable substitution for `${user_prompt}` and `${step_id.output...}`.
    Does NOT handle complex dependencies (assumes sequential execution based on list order).
    """
    logger.info(f"Executing dynamic tasks handler for task: {task.id}")
    task_definitions = input_data.get("task_definitions", [])
    user_prompt = input_data.get("user_prompt", "") # Get the user prompt

    if not isinstance(task_definitions, list):
        logger.error("Invalid input: 'task_definitions' is missing or not a list.")
        return {"success": False, "error": "Task definitions missing or not a list."}

    if not task_definitions:
        logger.info("No task definitions provided to execute.")
        return {"success": True, "result": {"execution_summary": "No tasks to execute.", "outputs": {}}}

    # Store full output results for each step as they complete
    all_step_outputs = {}
    execution_summary = []
    overall_success = True # Assume success unless a task fails

    # Variable pattern to match ${...}
    var_pattern = re.compile(r'\${(.*?)}')

    # Simple sequential execution simulation
    # Get registries via services to ensure we use the test instances
    services = get_services()
    tool_registry = services.tool_registry
    handler_registry = services.handler_registry

    for i, task_def in enumerate(task_definitions):
        step_number = i + 1
        task_id = task_def.get("task_id", f"dynamic_task_{step_number}")
        task_name = task_def.get("name", f"Dynamic Task {step_number}")
        step_type = task_def.get("task_class") if task_def.get("task_class") == "DirectHandlerTask" else task_def.get("type", "tool") # Infer type
        capability_name = task_def.get("tool_name") or task_def.get("handler_name")
        step_input_data = task_def.get("input_data", {})

        logger.info(f"--- Simulating execution of Step {step_number}: {task_id} ({task_name}) ---")
        logger.debug(f"Type: {step_type}, Capability: {capability_name}, Raw Input: {step_input_data}")

        # --- Enhanced Variable Substitution ---
        processed_input_data = {}
        try:
            for key, value in step_input_data.items():
                resolved_value = value # Default to original value
                if isinstance(value, str):
                     match = var_pattern.fullmatch(value)
                     if match:
                         var_name = match.group(1)
                         logger.debug(f"Found variable '{var_name}' in input key '{key}'")
                         # 1. Check for user_prompt
                         if var_name == "user_prompt":
                             resolved_value = user_prompt
                             logger.debug(f"Resolved to user_prompt.")
                         # 2. Check for previous task output (e.g., task_id.output.field.subfield)
                         elif '.output.' in var_name:
                             parts = var_name.split('.output.', 1)
                             source_task_id = parts[0]
                             field_path = parts[1]
                             logger.debug(f"Attempting to resolve from task '{source_task_id}' with path '{field_path}'")

                             if source_task_id in all_step_outputs:
                                 source_output_dict = all_step_outputs[source_task_id]
                                 # Navigate the dictionary using the field path
                                 current_val = source_output_dict
                                 path_valid = True
                                 try:
                                     for field in field_path.split('.'):
                                         if isinstance(current_val, dict):
                                             current_val = current_val.get(field)
                                             if current_val is None: # Field not found in dict
                                                  path_valid = False
                                                  break
                                         else: # Cannot navigate further
                                             path_valid = False
                                             break
                                     if path_valid:
                                         resolved_value = current_val
                                         logger.debug(f"Resolved successfully from {source_task_id}.")
                                     else:
                                         logger.warning(f"Could not resolve path '{field_path}' within output of task '{source_task_id}'. Using None.")
                                         resolved_value = None
                                 except Exception as nav_e:
                                     logger.warning(f"Error navigating output path for {var_name}: {nav_e}. Using None.")
                                     resolved_value = None
                             else:
                                 logger.warning(f"Source task '{source_task_id}' for variable '{var_name}' not found or not yet executed. Using None.")
                                 resolved_value = None
                         else:
                              logger.warning(f"Variable '{var_name}' format not recognized for substitution. Using literal value.")
                              resolved_value = value # Keep original string if format unknown
                # Store the processed value (could be original, substituted, or None)
                processed_input_data[key] = resolved_value

        except Exception as sub_e:
             logger.exception(f"Error during input substitution for step {task_id}: {sub_e}")
             processed_input_data = step_input_data # Fallback to original data on error

        logger.debug(f"Processed Input: {processed_input_data}")
        # --- End Substitution ---

        step_result = {}
        step_success = False

        try:
            if step_type == "tool":
                # Check existence using the registry from services
                if not tool_registry.get_tool(capability_name):
                     raise ValueError(f"Tool '{capability_name}' not found in registry provided by services.")
                logger.info(f"Executing tool: {capability_name}")
                # Execute using the registry from services
                step_result = tool_registry.execute_tool(capability_name, processed_input_data)
                step_success = step_result.get("success", False)

            elif step_type == "DirectHandlerTask" or step_type == "handler":
                 # Check existence using the registry from services
                 handler_func = handler_registry.get_handler(capability_name) # get_handler returns None if not found
                 if handler_func is None:
                     raise ValueError(f"Handler '{capability_name}' not found in registry provided by services.")
                 if not callable(handler_func):
                     raise ValueError(f"Retrieved handler '{capability_name}' is not callable.")

                 logger.info(f"Executing handler: {capability_name}")
                 mock_task_obj = type('obj', (object,), {'id': task_id, 'name': task_name})()
                 step_result = handler_func(mock_task_obj, processed_input_data)
                 step_success = step_result.get("success", False)
            else:
                 raise ValueError(f"Unsupported task type '{step_type}' for dynamic execution.")

            # Store the *entire* result dictionary for this step
            all_step_outputs[task_id] = step_result

            if step_success:
                logger.info(f"Step {task_id} completed successfully.")
                # Use result field for summary preview if available
                output_preview_content = step_result.get('result', step_result) # Fallback to full result
                execution_summary.append({"task_id": task_id, "status": "completed", "output_preview": str(output_preview_content)[:100] + "..."})
            else:
                 logger.warning(f"Step {task_id} failed.")
                 error_msg = step_result.get("error", "Unknown error")
                 execution_summary.append({"task_id": task_id, "status": "failed", "error": error_msg})
                 overall_success = False # Mark overall as failed

        except Exception as e:
            logger.exception(f"Error during simulated execution of step {task_id}: {e}")
            error_msg = f"Simulation execution error: {e}"
            # Store error in the output structure
            all_step_outputs[task_id] = {"success": False, "error": error_msg}
            execution_summary.append({"task_id": task_id, "status": "failed", "error": error_msg})
            overall_success = False

    logger.info("--- Dynamic Task Execution Simulation Complete ---")

    return {
        "success": overall_success,
        "result": {
            "execution_summary": execution_summary,
            "outputs": all_step_outputs # Dictionary mapping task_id to its full output dict
        },
        "error": None if overall_success else "One or more dynamic tasks failed during simulated execution."
    }


def summarize_results_handler(task: DirectHandlerTask, input_data: dict) -> dict:
    """
    Summarizes the results of the dynamic task execution simulation.
    """
    logger.info(f"Executing summarize results handler for task: {task.id}")
    # Access the dictionary of full step outputs
    all_step_outputs = input_data.get("execution_outputs", {})
    execution_summary = input_data.get("execution_summary", []) # Summary from simulation handler
    overall_success = input_data.get("overall_success", True) # Overall success from simulation handler

    final_message = "Workflow execution simulation summary:\n"
    if execution_summary:
         for summary_item in execution_summary:
             task_id = summary_item.get('task_id', 'Unknown')
             status = summary_item.get('status', 'Unknown')
             # Get preview from summary, but could also access full output via all_step_outputs[task_id]
             preview = summary_item.get('output_preview') or summary_item.get('error')
             final_message += f"- Task '{task_id}': {status}"
             if preview:
                 final_message += f" -> {preview}\n"
             else:
                 final_message += "\n"
    else:
        final_message += "- No dynamic tasks were executed or summary is unavailable.\n"

    if not overall_success:
        final_message += "\nNote: One or more tasks failed during execution."

    logger.info(f"Final Summary: {final_message}")

    # This result is typically what the Agent/Engine returns
    return {
        "success": overall_success, # Reflect the success of the dynamic execution
        "result": {
            "final_summary": final_message,
            "raw_outputs": all_step_outputs # Pass through the full outputs
        }
    }


# --- Workflow Definition ---

def build_chat_planner_workflow() -> Workflow:
    """
    Builds the chat-driven planning workflow.
    """
    workflow = Workflow(
        workflow_id="chat_planner_v1",
        name="Chat-Driven Planning Workflow"
    )

    # --- Phase 1: Input (Handled by Workflow Engine via initial context) ---
    # Initial input expected: {"user_prompt": "User's request..."}

    # --- Phase 1.5: Get Available Capabilities ---
    get_capabilities_task = Task(
        task_id="get_capabilities",
        name="Get Available Tools and Handlers",
        tool_name="get_available_capabilities", # Call the new tool
        input_data={}, # Tool currently takes no input
        next_task_id_on_success="think_analyze_plan"
    )
    workflow.add_task(get_capabilities_task)

    # --- Phase 2: Plan Generation ("Think & Analyze") ---
    # Use DirectHandlerTask with the handler defined above
    plan_task = DirectHandlerTask(
        task_id="think_analyze_plan",
        name="Generate Execution Plan",
        handler=plan_user_request_handler, # Reference the actual handler function
        input_data={
            "user_request": "${user_prompt}",
            # Use output from the get_capabilities task
            "available_tools_context": "${get_capabilities.output.result.tools_context}",
            "available_handlers_context": "${get_capabilities.output.result.handlers_context}"
        },
        # Depends on get_capabilities_task implicitly via next_task_id
        # next_task_id_on_success="validate_plan" # If validation step is added
        next_task_id_on_success="validate_plan" # Changed to point to validation
    )
    workflow.add_task(plan_task)

    # --- Phase 2.5: Plan Validation ---
    validate_plan_task = DirectHandlerTask(
        task_id="validate_plan",
        name="Validate Generated Plan",
        handler=validate_plan_handler,
        input_data={
            # Get raw output from the planning task
            "raw_llm_output": "${think_analyze_plan.output.result.raw_llm_output}",
            # Get tool/handler details from the capabilities task
            "tool_details": "${get_capabilities.output.result.tool_details}",
            "handler_details": "${get_capabilities.output.result.handler_details}"
        },
        next_task_id_on_success="generate_tasks", # Proceed if valid
        next_task_id_on_failure="handle_plan_failure" # Go to failure handler if invalid
    )
    workflow.add_task(validate_plan_task)

    # --- Phase 3: Dynamic Task Generation ---
    # Placeholder - requires PlanToTasksHandler implementation
    generate_tasks_task = DirectHandlerTask(
        task_id="generate_tasks",
        name="Generate Tasks From Plan",
        handler=plan_to_tasks_handler, # Use the actual handler
        input_data={
            "plan": "${validate_plan.output.result.validated_plan}" # Use the validated plan
        },
        # Depends on validate_plan implicitly
        next_task_id_on_success="execute_dynamic_tasks", # Placeholder for execution step
        next_task_id_on_failure="handle_plan_failure" # Go to failure handler if conversion fails
    )
    workflow.add_task(generate_tasks_task)

    # --- Phase 4: Dynamic Task Execution ---
    # Placeholder - requires dynamic execution mechanism
    execute_task = DirectHandlerTask(
        task_id="execute_dynamic_tasks",
        name="Execute Generated Tasks (Simulation)",
        handler=execute_dynamic_tasks_handler,
        input_data={
            "task_definitions": "${generate_tasks.output.result.task_definitions}",
            # Pass the original prompt for basic variable substitution
            "user_prompt": "${user_prompt}"
        },
        next_task_id_on_success="summarize_results",
        next_task_id_on_failure="handle_plan_failure"
    )
    workflow.add_task(execute_task)

    # --- Phase 5: Output ---
    # Placeholder - requires SummarizeResults handler/task
    summarize_task = DirectHandlerTask(
        task_id="summarize_results",
        name="Summarize Execution Results",
        handler=summarize_results_handler,
        input_data={
            "execution_outputs": "${execute_dynamic_tasks.output.result.outputs}",
            "execution_summary": "${execute_dynamic_tasks.output.result.execution_summary}",
            "overall_success": "${execute_dynamic_tasks.output.success}"
        }
    )
    workflow.add_task(summarize_task)

    # --- Failure Handling Task ---
    handle_failure_task = DirectHandlerTask(
        task_id="handle_plan_failure",
        name="Handle Plan Generation/Validation Failure",
        handler=lambda task, data: {
            "success": False, # Indicate overall workflow failure
            "error": "Plan generation or validation failed.",
            "details": data # Pass through the error details from previous step
            },
        # This is a terminal task in the failure path
    )
    workflow.add_task(handle_failure_task)

    logger.info(f"Workflow '{workflow.id}' created with {len(workflow.tasks)} tasks.")
    return workflow

# --- Mock/Example Tools/Handlers for Planning ---
def mock_search_tool(input_data):
    query = input_data.get("query", "")
    logger.info(f"MOCK Search Tool: Searching for '{query}'")
    # Simulate finding some results
    return {"success": True, "result": f"Found 3 documents related to '{query}'."}

def mock_summarize_handler(task, input_data):
    text = input_data.get("text", "")
    logger.info(f"MOCK Summarize Handler: Summarizing text (first 50 chars): '{text[:50]}...'")
    return {"success": True, "result": f"Summary of '{text[:20]}...'"}

# --- Mock LLM Interface ---
class MockLLMInterface(LLMInterface):
    def __init__(self, plan_response: str):
        self._plan_response = plan_response

    def execute_llm_call(self, prompt: str, **kwargs) -> Dict[str, Any]:
        # Simulate LLM call - return a pre-defined plan for testing
        logger.info("MOCK LLM: Returning pre-defined plan.")
        # Extract user prompt from the full prompt for logging/context
        user_prompt_match = re.search(r"\*\*User Request:\*\*\s*```\s*(.*?)\s*```", prompt, re.DOTALL)
        user_prompt_extracted = user_prompt_match.group(1).strip() if user_prompt_match else "Unknown"

        # Example Plan - Adjust based on mock tools/handlers
        # This plan uses the mock search tool and mock summarize handler
        # Define as a regular triple-quoted string, not an f-string
        mock_plan_json = """
        [
          {
            "step_id": "step_1_search",
            "description": "Search for documents based on the user request.",
            "type": "tool",
            "name": "mock_search",
            "inputs": { "query": "${user_prompt}" },
            "outputs": ["search_results"],
            "depends_on": []
          },
          {
            "step_id": "step_2_summarize",
            "description": "Summarize the search results found.",
            "type": "handler",
            "name": "mock_summarize",
            "inputs": { "text": "${step_1_search.output.result}" },
             "outputs": ["summary"],
            "depends_on": ["step_1_search"]
          }
        ]
        """
        # In a real scenario, this would be the actual LLM response string
        return {"success": True, "response": mock_plan_json}

def main():
    """Main function to build and TEST the workflow."""
    logger.info("--- Setting up Test Environment for Chat Planner Workflow ---")

    # 1. Reset and Get Global Services Container
    reset_services() # Clear any previous state
    services = get_services() # Get the global instance

    # 2. Create and Register Test-Specific Registries
    test_tool_registry = ToolRegistry()
    test_handler_registry = HandlerRegistry()
    services.register_tool_registry(test_tool_registry) # Use this specific instance globally for the test
    services.register_handler_registry(test_handler_registry)

    # 3. Register Mock LLM
    # Use the mock LLM for testing
    services.register_llm_interface(MockLLMInterface(plan_response="")) # Plan passed in execute call

    # 4. Register Framework Tool and Handlers into Test Registries
    # Register tool directly to the test registry instance
    if not test_tool_registry.get_tool("get_available_capabilities"):
        test_tool_registry.register_tool("get_available_capabilities", get_available_capabilities)

    # Register handlers directly to the test registry instance
    workflow_handlers = {
        "plan_user_request": plan_user_request_handler,
        "validate_plan": validate_plan_handler,
        "plan_to_tasks": plan_to_tasks_handler,
        "execute_dynamic_tasks": execute_dynamic_tasks_handler,
        "summarize_results": summarize_results_handler,
        "mock_summarize": mock_summarize_handler
    }
    for name, handler in workflow_handlers.items():
         test_handler_registry.register_handler(name, handler)

    # Register mock tool used by the test plan directly to the test registry
    if not test_tool_registry.get_tool("mock_search"):
        test_tool_registry.register_tool("mock_search", mock_search_tool)

    # Access the underlying dictionary keys to list tools
    logger.info(f"Tools Registered in Test Registry: {list(test_tool_registry.tools.keys())}")
    # Similarly, assume handlers are stored in a dict named _handlers or handlers
    # Use list_handlers() from registry_access if it correctly reflects the registry state
    registered_handler_names = list(test_handler_registry.list_handlers()) # Assuming list_handlers exists and works
    logger.info(f"Handlers Registered in Test Registry: {registered_handler_names}")

    # 5. Build Workflow
    logger.info("Building the Chat Planner Workflow...")
    workflow = build_chat_planner_workflow()
    logger.info(f"Workflow '{workflow.id}' built.")

    # 5.5 Generate Visualization (Bonus)
    try:
        viz_output_file = "chat_planner_workflow" # Output file name (base)
        logger.info(f"Attempting to generate workflow visualization: {viz_output_file}.pdf/png")
        # Call the correct function, adjust parameters if needed (e.g., filename without ext, format)
        visualize_workflow(workflow, filename=viz_output_file.replace(".gv", ""), format='pdf', view=False)
        logger.info(f"Visualization DOT file ({viz_output_file}) generated successfully.")
        # To convert DOT to PNG: dot -Tpng chat_planner_workflow.gv -o chat_planner_workflow.png
    except ImportError:
        logger.warning("Could not import visualize_workflow. Skipping visualization.")
        logger.warning("Install graphviz library (`pip install graphviz`) to enable visualization.")
    except FileNotFoundError:
        logger.error("Graphviz `dot` command not found. Install Graphviz (https://graphviz.org/download/) to generate images.")
    except Exception as viz_e:
        logger.error(f"Failed to generate workflow visualization: {viz_e}")

    # 6. Prepare Initial Context
    test_user_prompt = "Find information about project Dawn and summarize it."
    initial_context = {"user_prompt": test_user_prompt}

    # 7. Instantiate Engine and Run (Simplified Sync Execution for Testing)
    logger.info("--- Starting Synchronous Workflow Test Execution ---")
    try:
        # --- Find the starting task dynamically ---
        all_task_ids = set(workflow.tasks.keys())
        target_task_ids = set()
        for task in workflow.tasks.values():
            if task.next_task_id_on_success:
                target_task_ids.add(task.next_task_id_on_success)
            if task.next_task_id_on_failure:
                target_task_ids.add(task.next_task_id_on_failure)
        
        start_task_ids = list(all_task_ids - target_task_ids)
        
        if not start_task_ids:
            raise RuntimeError("Could not determine the start task for the workflow.")
        if len(start_task_ids) > 1:
            logger.warning(f"Multiple potential start tasks found: {start_task_ids}. Using the first one: {start_task_ids[0]}")
            
        current_task_id = start_task_ids[0]
        logger.info(f"Determined start task ID: {current_task_id}")
        # --- End finding start task ---

        context = initial_context.copy()
        max_steps = len(workflow.tasks) + 5 # Safety break
        steps_taken = 0

        while current_task_id and steps_taken < max_steps:
            steps_taken += 1
            task = workflow.tasks.get(current_task_id)
            if not task:
                logger.error(f"Task ID '{current_task_id}' not found in workflow. Stopping.")
                break

            logger.info(f"\n>>> Executing Task: {task.id} ({task.name})")

            # Resolve input data (basic simulation)
            task_input_data = {}
            if isinstance(task.input_data, dict):
                for key, value in task.input_data.items():
                     if isinstance(value, str) and value.startswith("${") and value.endswith("}"):
                         var_name = value[2:-1]
                         # Super basic resolution: check context first
                         resolved_value = context.get(var_name)
                         # Attempt slightly more complex resolution for task outputs (basic)
                         if resolved_value is None and '.' in var_name:
                             parts = var_name.split('.', 2)
                             if len(parts) == 3 and parts[1] == 'output':
                                 source_task_id = parts[0]
                                 field_path = parts[2]
                                 source_output = context.get(f"{source_task_id}.output")
                                 if isinstance(source_output, dict):
                                     # Basic nested access (e.g., result.validated_plan)
                                     current_val = source_output
                                     try:
                                         for field in field_path.split('.'):
                                             if isinstance(current_val, dict):
                                                 current_val = current_val.get(field)
                                             else:
                                                 current_val = None
                                                 break
                                         resolved_value = current_val
                                     except Exception:
                                         resolved_value = None # Failed to resolve path

                         if resolved_value is None:
                             logger.warning(f"Could not resolve variable '{value}' for task {task.id}. Using None.")
                         task_input_data[key] = resolved_value
                         logger.debug(f"Resolved '{value}' to: {str(resolved_value)[:50]}...")
                     else:
                         task_input_data[key] = value
            else:
                 task_input_data = task.input_data or {}

            logger.debug(f"Task Input Data: {task_input_data}")

            # Execute Task (Tool or Handler) - Ensure execute_tool/get_handler are used correctly
            task_result = {}
            task_success = False
            if isinstance(task, DirectHandlerTask):
                handler_func = task.handler # Get handler directly from task object
                if callable(handler_func):
                    logger.debug(f"Executing Direct Handler: {handler_func.__name__ if hasattr(handler_func, '__name__') else 'lambda'}")
                    task_result = handler_func(task, task_input_data)
                else:
                     task_result = {"success": False, "error": "Handler in DirectHandlerTask is not callable."}
            elif task.tool_name:
                 logger.debug(f"Executing Tool: {task.tool_name}")
                 # Use the specific test_tool_registry instance directly
                 if test_tool_registry.get_tool(task.tool_name):
                     task_result = test_tool_registry.execute_tool(task.tool_name, task_input_data)
                 else:
                     task_result = {"success": False, "error": f"Tool '{task.tool_name}' not found in test registry."}
            else:
                 logger.error(f"Task {task.id} has no tool or handler. Skipping.")
                 task_result = {"success": False, "error": "Task misconfigured."}

            task_success = task_result.get("success", False)
            logger.info(f"Task {task.id} Execution Result: Success={task_success}")
            if not task_success:
                logger.error(f"Task {task.id} failed: {task_result.get('error', 'No error message.')}")
            logger.debug(f"Task Output: {task_result}")

            # Store output in context
            context[f"{task.id}.output"] = task_result

            # Determine next task
            if task_success:
                 current_task_id = task.next_task_id_on_success
                 logger.info(f"Next task on success: {current_task_id}")
            else:
                 current_task_id = task.next_task_id_on_failure
                 logger.info(f"Next task on failure: {current_task_id}")

            if not current_task_id:
                 logger.info("Workflow execution finished.")

        if steps_taken >= max_steps:
            logger.error("Maximum execution steps reached. Stopping.")

        logger.info("\n--- Final Workflow Context (Selected Items) ---")
        final_summary = context.get("summarize_results.output", {}).get("result", {}).get("final_summary", "Summary not generated.")
        logger.info(f"Final Summary:\n{final_summary}")


    except Exception as e:
        logger.exception(f"Error during workflow test execution: {e}")

    print("\n--- Test Execution Complete ---")


if __name__ == "__main__":
    main() 