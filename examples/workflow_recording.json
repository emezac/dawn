{
  "version": "1.0",
  "created_at": "2025-04-12T00:45:01.477403",
  "recordings": [
    {
      "tool_name": "file_system",
      "inputs": {
        "path": "README.md",
        "operation": "read"
      },
      "output": {
        "success": true,
        "content": "# AI Agent Framework\n\nThe AI Agent Framework is designed to enhance the capabilities of AI agents, allowing them to perform tasks such as web searching, file reading, file uploading, and markdown file creation. Recent updates include support for conditional workflows, enhancements to the web search tool, and the ability to visualize workflow execution.\n\n## Features\n\n- **Conditional Workflows**: Enable dynamic task execution based on specific conditions or task outcomes, enhancing flexibility and adaptability.\n- **Web Search Tool**: Updated to use the latest model version with improved performance and reliability.\n- **Visualization of Workflow Execution**: Generate visual representations of workflows to aid in understanding and debugging.\n- **Vector Store ID Validation**: Robust utilities for validating OpenAI Vector Store IDs with different validation levels.\n- **Error Propagation Between Tasks**: Comprehensive error tracking and propagation system allowing downstream tasks to access and handle errors from upstream tasks.\n- **DirectHandlerTask Support:**\n  - Register and use handler functions directly in workflows without needing global tool registry.\n  - Improved task output and variable resolution for complex data structures.\n  - Support for both synchronous and asynchronous workflow engines.\n  - Enhanced debugging and error reporting for task execution.\n- **Observability:**\n  - Logging and tracing to support debugging and workflow analysis.\n- **Services Container:**\n  - Centralized dependency management for framework-wide singletons\n  - Type-safe access to shared services like ToolRegistry and LLMInterface\n  - Proper dependency injection support throughout the framework\n  - Simplified management of service lifecycle and configuration\n\n## Usage\n\n### Conditional Workflows\n\nConditional workflows allow for dynamic task execution based on specific conditions or task outcomes. This feature enhances the framework's flexibility and adaptability.\n\n#### Usage Example\n\n```python\nworkflow = [\n    {\n        \"id\": \"task_1\",\n        \"tool\": \"llm_generate_idea\",\n        \"condition\": None\n    },\n    {\n        \"id\": \"task_2a\",\n        \"tool\": \"draft_email\",\n        \"condition\": {\n            \"depends_on\": \"task_1\",\n            \"if_result\": \"good\"\n        }\n    },\n    ...\n]\n```\n\n### Web Search Tool\n\nThe web search tool has been updated to use the latest model version and includes a timeout setting for improved performance.\n\n#### Usage Example\n\n```python\ninput_data = {\n    \"query\": \"What was a positive news story from today?\",\n    \"context_size\": \"medium\"\n}\nresult = registry.execute_tool(\"web_search\", input_data)\n```\n\n### Visualization of Workflow Execution\n\nThe framework supports generating visual representations of workflows using Graphviz, which helps in understanding and debugging complex workflows.\n\n#### Usage Example\n\n```python\nfrom core.utils.visualizer import visualize_workflow\n\n# Assuming 'workflow' is an instance of the Workflow class\nvisualize_workflow(workflow, filename=\"workflow_graph\", format='pdf', view=True)\n```\n\n### Vector Store ID Validation\n\nThe framework provides utilities for validating OpenAI Vector Store IDs, ensuring consistent validation across all tools and workflows.\n\n#### Usage Example\n\n```python\nfrom tools.openai_vs.utils.vs_id_validator import (\n    is_valid_vector_store_id, \n    is_strict_valid_vector_store_id,\n    validate_vector_store_id,\n    assert_valid_vector_store_id\n)\n\n# Simple boolean validation\nif is_valid_vector_store_id(vector_store_id):\n    # Perform operation\n\n# Strict validation with regex pattern\nif is_strict_valid_vector_store_id(vector_store_id):\n    # Perform operation requiring strict format\n\n# Get validation status and error message\nis_valid, error_message = validate_vector_store_id(vector_store_id, strict=True)\nif not is_valid:\n    print(f\"Invalid vector store ID: {error_message}\")\n\n# Assert valid ID (raises ValueError if invalid)\ntry:\n    assert_valid_vector_store_id(vector_store_id, strict=False)\n    # Continue with operation\nexcept ValueError as e:\n    print(f\"Validation error: {e}\")\n```\n\nFor more details on vector store ID validation, see the [Vector Store ID Validation documentation](docs/vector_store_id_validation.md).\n\n### Error Propagation Between Tasks\n\nThe framework now supports robust error propagation between tasks, allowing workflows to implement sophisticated error handling and recovery strategies.\n\n#### Usage Example\n\n```python\n# Define a task that might fail\ndata_processing_task = Task(\n    task_id=\"process_data\",\n    name=\"Process Data\",\n    tool_name=\"data_processor\",\n    input_data={\"data_source\": \"customer_data.csv\"},\n    next_task_id_on_success=\"analyze_data\",  # Normal path on success\n    next_task_id_on_failure=\"handle_error\"   # Error handling path on failure\n)\n\n# Define an error handler task\nerror_handler_task = Task(\n    task_id=\"handle_error\",\n    name=\"Error Handler\",\n    tool_name=\"error_recovery_tool\",\n    input_data={\n        # Access error information from the failed task\n        \"error_message\": \"${error.process_data}\",\n        \"error_code\": \"${error.process_data.error_code}\",\n        \"error_details\": \"${error.process_data.error_details}\",\n        \"original_input\": \"customer_data.csv\"\n    },\n    next_task_id_on_success=\"retry_processing\",  # Try again if recovery succeeded\n    next_task_id_on_failure=\"report_failure\"      # Report failure if recovery fails\n)\n\n# Add tasks to the workflow\nworkflow.add_task(data_processing_task)\nworkflow.add_task(error_handler_task)\n```\n\nThe error propagation system automatically tracks errors across tasks and provides:\n- Detailed error information with standardized format\n- Error references in task inputs using the `${error.task_id}` syntax\n- Error propagation chains to track error origins\n- Workflow-level error summaries for monitoring and analysis\n\nFor more details, see the [Error Propagation documentation](docs/error_propagation.md).\n\n### DirectHandlerTask Support\n\n```python\nfrom core.task import DirectHandlerTask\n\n# Define a handler function with the new two-parameter format\ndef my_custom_handler(task, input_data):\n    # Access task properties if needed\n    print(f\"Executing task: {task.name} (ID: {task.id})\")\n    \n    # Process the input data\n    result = process_data(input_data[\"value\"])\n    \n    return {\"success\": True, \"result\": result}\n\n# Create a task with the direct handler\ntask = DirectHandlerTask(\n    task_id=\"custom_processor\",\n    name=\"Custom Data Processor\",\n    handler=my_custom_handler,\n    input_data={\"value\": \"sample_data\"}\n)\n\n# Add to workflow\nworkflow.add_task(task)\n```\n\n> **Note:** DirectHandlerTask now supports both older single-parameter handlers (just `input_data`) and the new two-parameter format (`task, input_data`). The two-parameter format is recommended as it provides access to the task context.\n\n### Services Container\n\nThe Services Container provides centralized management of framework dependencies and ensures consistent access to singleton services.\n\n```python\nfrom core.services import get_services\n\n# Get the services container\nservices = get_services()\n\n# Access the tool registry (singleton)\nregistry = services.tool_registry\n\n# Register a custom LLM interface\nfrom core.llm.interface import LLMInterface\nllm = LLMInterface(model=\"gpt-4\")\nservices.register_llm_interface(llm, name=\"gpt4_interface\")\n\n# Create an agent using services\nagent = Agent(\n    agent_id=\"my_agent\",\n    name=\"My Agent\",\n    # The registry will be automatically provided from the services container\n    # Use a specific named LLM interface\n    llm_interface=services.get_llm_interface(\"gpt4_interface\")\n)\n```\n\n## Examples\n\n- **Complex Conditional Workflow**: `examples/complex_conditional_workflow.py`\n- **Simple Conditional Workflow**: `examples/simple_conditional_workflow.py`\n- **Complex Workflow**: `examples/complex_workflow.py`\n- **Vector Store Example**: `examples/vector_store_example.py`\n\n## Overview\n\nThe AI Agent Framework is an open-source Python framework designed to simplify the development of agent-based applications. It features a robust and explicit system for dynamic workflow management, allowing agents to break down complex tasks, execute sub-tasks, evaluate results, and dynamically adjust their action plans. This improves agent reliability and adaptability.\n\n## Features\n\n- **Dynamic Workflow Management System (WMS):**\n  - Explicit task representation with state, dependencies, and tools.\n  - Definition of workflows (sequential, parallel, conditional).\n  - Monitored task execution with feedback integration for decision-making.\n  - Workflow state tracking.\n  - **Visualization of Workflow Execution:** Generate visual representations of workflows to aid in understanding and debugging.\n\n- **LLM Interface:**\n  - Compatibility with popular APIs (e.g., OpenAI).\n  - Simple request/response pattern with function/tool calling.\n\n- **Tool Interface:**\n  - Easy integration of custom tools.\n  - Example tools include a calculator for arithmetic operations.\n\n- **Web Search Tool:**\n  - Allows models to search the web for the latest information.\n  - Configured in the `tools` array of an API request.\n  - Supports user location and search context size customization.\n\n- **Vector Store Tools:**\n  - Create and manage OpenAI Vector Stores for efficient data storage and retrieval.\n  - Utilities for validating vector store IDs with basic and strict validation options.\n  - Integration with file uploads and text storage for comprehensive knowledge management.\n\n- **DirectHandlerTask Support:**\n  - Register and use handler functions directly in workflows without needing global tool registry.\n  - Improved task output and variable resolution for complex data structures.\n  - Support for both synchronous and asynchronous workflow engines.\n  - Enhanced debugging and error reporting for task execution.\n\n- **Observability:**\n  - Logging and tracing to support debugging and workflow analysis.\n\n## Dependencies\n\nThe framework has the following core dependencies:\n\n- **OpenAI API**: For LLM access and vector store operations\n- **HTTPX**: For direct API calls when the OpenAI client doesn't support certain operations\n- **Pydantic**: For data validation and settings management\n- **Graphviz** (optional): For workflow visualization\n- **python-dotenv**: For environment variable management\n\n## Installation\n\n1. **Clone the Repository:**\n\n   ```bash\n   git clone <repository-url>\n   cd <repository-directory>\n   ```\n\n2. **Set Up Virtual Environment:**\n\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n   ```\n\n3. **Install Dependencies:**\n\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. **Install Development Dependencies (optional):**\n\n   ```bash\n   pip install -r requirements-dev.txt\n   ```\n\n5. **Set Up Environment Variables:**\n   - Create a `.env` file in the root directory.\n   - Add your OpenAI API key:\n\n     ```\n     OPENAI_API_KEY=your_openai_api_key\n     ```\n\n## Usage\n\n1. **Run the Example Workflow:**\n\n   ```bash\n   python examples/simple_workflow.py\n   ```\n\n2. **Define Your Own Workflow:**\n   - Create tasks using the `Task` class.\n   - Define workflows using the `Workflow` class.\n   - Use the `Agent` class to load and run workflows.\n\n3. **Integrate Web Search Tool:**\n   - Use the Web Search tool to enhance your agent's ability to access real-time information.\n   - Configure the tool in the `tools` array of your API requests.\n\n4. **Visualize Workflow Execution:**\n   - Use the `visualize_workflow` function to generate visual representations of your workflows.\n\n## Development\n\n### Code Style and Linting\n\nThis project uses several tools to maintain code quality:\n\n- **Black**: For code formatting (line length: 120 characters)\n- **isort**: For sorting imports (configured to work with Black)\n- **Flake8**: For linting and style checking\n\nThe project is configured with appropriate settings for each tool:\n- Line length is set to 120 characters for all tools\n- Specific file patterns have customized linting rules\n- Common errors are selectively ignored in example and test files\n\n#### Setup Pre-commit Hooks\n\n```bash\npre-commit install\n```\n\n#### Manual Linting\n\nYou can use the Makefile or the lint script for linting:\n\n```bash\n# Using Make\nmake format    # Run isort and black\nmake lint      # Run flake8\nmake lint-all  # Run both formatting and linting\n\n# Using the lint script\n./lint.sh\n```\n\n#### Running Tests\n\n```bash\nmake test      # Run all tests with pytest\nPYTHONPATH=.  python -m unittest tests/test_file_name.py  # Run a specific test file\n```\n\n#### Linting Configuration\n\nThe project includes the following configuration files:\n- `setup.cfg`: Contains settings for flake8 and isort\n  - Configures what errors to ignore globally and per file\n  - Ignores common docstring-related errors (D100-D107, D200, D205, D400, D401)\n- `pyproject.toml`: Contains settings for black\n- `.pre-commit-config.yaml`: Configures pre-commit hooks for automatic linting\n  - Includes flake8-docstrings as an additional dependency for pre-commit checks\n\n**Note on docstring checking**: When running `flake8` directly, it uses the configuration in `setup.cfg`. \nHowever, the pre-commit hook includes additional docstring checking through flake8-docstrings. \nThe setup is configured to ignore most docstring formatting errors to maintain a balance between \ncode quality and development speed.\n\n## Key Concepts\n\n- **Task:** Represents a unit of work with attributes like `id`, `name`, `status`, `input_data`, `output_data`, `is_llm_task`, and `tool_name`.\n- **Workflow:** A collection of tasks with a defined execution order and logic.\n- **Agent:** Manages workflows, LLM interfaces, and tool registries, providing methods to run workflows and get results.\n- **Tool Interface:** Allows for the registration and execution of custom tools, including the Web Search tool.\n\n## Development Plan\n\nThe project is structured into phases, with the initial version focusing on core functionality and basic examples. Future enhancements may include more complex multi-agent orchestration and advanced state persistence.\n\n## Contributing\n\nContributions are welcome! Please fork the repository and submit a pull request with your changes. Ensure that your code adheres to the project's coding standards and includes appropriate tests.\n\nBefore submitting a pull request:\n1. Run the linting tools to ensure code quality\n2. Make sure all tests pass\n3. Update documentation if necessary\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n## Contact\n\nFor questions or feedback, please contact Enrique Meza C: emezac at [gmail.com]\n\n## Documentation\n\nThe project includes comprehensive documentation:\n\n- **Usage Guides**: Examples and tutorials on how to use the framework effectively.\n- **API Reference**: Documentation of all public classes, methods, and interfaces.\n- **Tool Documentation**: Detailed information on available tools and their usage.\n- **Validation Utilities**: Guidelines for validating inputs and outputs, including [Vector Store ID Validation](docs/vector_store_id_validation.md).\n- **OpenAI Integration**: Information on working with OpenAI APIs, including:\n  - [File Purpose Parameters](docs/openai_file_purpose_parameters.md)\n  - [API Parameter Changes](docs/openai_api_parameter_changes.md) (including Beta API header format)\n- **Fixes and Enhancements**: Documentation of known issues and their solutions in the `fixes` directory.\n",
        "status": "success",
        "timestamp": "2025-04-12T00:45:01.474997"
      },
      "success": true,
      "timestamp": "2025-04-12T00:45:01.475009"
    },
    {
      "tool_name": "openai",
      "inputs": {
        "prompt": "Please extract all entities (people, organizations, locations) from the following document:\n\n# AI Agent Framework\n\nThe AI Agent Framework is designed to enhance the capabilities of AI agents, allowing them to perform tasks such as web searching, file reading, file uploading, and markdown file creation. Recent updates include support for conditional workflows, enhancements to the web search tool, and the ability to visualize workflow execution.\n\n## Features\n\n- **Conditional Workflows**: Enable dynamic task execution based on specific conditions or task outcomes, enhancing flexibility and adaptability.\n- **Web Search Tool**: Updated to use the latest model version with improved performance and reliability.\n- **Visualization of Workflow Execution**: Generate visual representations of workflows to aid in understanding and debugging.\n- **Vector Store ID Validation**: Robust utilities for validating OpenAI Vector Store IDs with different validation levels.\n- **Error Propagation Between Tasks**: Comprehensive error tracking and propagation system allowing downstream tasks to access and handle errors from upstream tasks.\n- **DirectHandlerTask Support:**\n  - Register and use handler functions directly in workflows without needing global tool registry.\n  - Improved task output and variable resolution for complex data structures.\n  - Support for both synchronous and asynchronous workflow engines.\n  - Enhanced debugging and error reporting for task execution.\n- **Observability:**\n  - Logging and tracing to support debugging and workflow analysis.\n- **Services Container:**\n  - Centralized dependency management for framework-wide singletons\n  - Type-safe access to shared services like ToolRegistry and LLMInterface\n  - Proper dependency injection support throughout the framework\n  - Simplified management of service lifecycle and configuration\n\n## Usage\n\n### Conditional Workflows\n\nConditional workflows allow for dynamic task execution based on specific conditions or task outcomes. This feature enhances the framework's flexibility and adaptability.\n\n#### Usage Example\n\n```python\nworkflow = [\n    {\n        \"id\": \"task_1\",\n        \"tool\": \"llm_generate_idea\",\n        \"condition\": None\n    },\n    {\n        \"id\": \"task_2a\",\n        \"tool\": \"draft_email\",\n        \"condition\": {\n            \"depends_on\": \"task_1\",\n            \"if_result\": \"good\"\n        }\n    },\n    ...\n]\n```\n\n### Web Search Tool\n\nThe web search tool has been updated to use the latest model version and includes a timeout setting for improved performance.\n\n#### Usage Example\n\n```python\ninput_data = {\n    \"query\": \"What was a positive news story from today?\",\n    \"context_size\": \"medium\"\n}\nresult = registry.execute_tool(\"web_search\", input_data)\n```\n\n### Visualization of Workflow Execution\n\nThe framework supports generating visual representations of workflows using Graphviz, which helps in understanding and debugging complex workflows.\n\n#### Usage Example\n\n```python\nfrom core.utils.visualizer import visualize_workflow\n\n# Assuming 'workflow' is an instance of the Workflow class\nvisualize_workflow(workflow, filename=\"workflow_graph\", format='pdf', view=True)\n```\n\n### Vector Store ID Validation\n\nThe framework provides utilities for validating OpenAI Vector Store IDs, ensuring consistent validation across all tools and workflows.\n\n#### Usage Example\n\n```python\nfrom tools.openai_vs.utils.vs_id_validator import (\n    is_valid_vector_store_id, \n    is_strict_valid_vector_store_id,\n    validate_vector_store_id,\n    assert_valid_vector_store_id\n)\n\n# Simple boolean validation\nif is_valid_vector_store_id(vector_store_id):\n    # Perform operation\n\n# Strict validation with regex pattern\nif is_strict_valid_vector_store_id(vector_store_id):\n    # Perform operation requiring strict format\n\n# Get validation status and error message\nis_valid, error_message = validate_vector_store_id(vector_store_id, strict=True)\nif not is_valid:\n    print(f\"Invalid vector store ID: {error_message}\")\n\n# Assert valid ID (raises ValueError if invalid)\ntry:\n    assert_valid_vector_store_id(vector_store_id, strict=False)\n    # Continue with operation\nexcept ValueError as e:\n    print(f\"Validation error: {e}\")\n```\n\nFor more details on vector store ID validation, see the [Vector Store ID Validation documentation](docs/vector_store_id_validation.md).\n\n### Error Propagation Between Tasks\n\nThe framework now supports robust error propagation between tasks, allowing workflows to implement sophisticated error handling and recovery strategies.\n\n#### Usage Example\n\n```python\n# Define a task that might fail\ndata_processing_task = Task(\n    task_id=\"process_data\",\n    name=\"Process Data\",\n    tool_name=\"data_processor\",\n    input_data={\"data_source\": \"customer_data.csv\"},\n    next_task_id_on_success=\"analyze_data\",  # Normal path on success\n    next_task_id_on_failure=\"handle_error\"   # Error handling path on failure\n)\n\n# Define an error handler task\nerror_handler_task = Task(\n    task_id=\"handle_error\",\n    name=\"Error Handler\",\n    tool_name=\"error_recovery_tool\",\n    input_data={\n        # Access error information from the failed task\n        \"error_message\": \"${error.process_data}\",\n        \"error_code\": \"${error.process_data.error_code}\",\n        \"error_details\": \"${error.process_data.error_details}\",\n        \"original_input\": \"customer_data.csv\"\n    },\n    next_task_id_on_success=\"retry_processing\",  # Try again if recovery succeeded\n    next_task_id_on_failure=\"report_failure\"      # Report failure if recovery fails\n)\n\n# Add tasks to the workflow\nworkflow.add_task(data_processing_task)\nworkflow.add_task(error_handler_task)\n```\n\nThe error propagation system automatically tracks errors across tasks and provides:\n- Detailed error information with standardized format\n- Error references in task inputs using the `${error.task_id}` syntax\n- Error propagation chains to track error origins\n- Workflow-level error summaries for monitoring and analysis\n\nFor more details, see the [Error Propagation documentation](docs/error_propagation.md).\n\n### DirectHandlerTask Support\n\n```python\nfrom core.task import DirectHandlerTask\n\n# Define a handler function with the new two-parameter format\ndef my_custom_handler(task, input_data):\n    # Access task properties if needed\n    print(f\"Executing task: {task.name} (ID: {task.id})\")\n    \n    # Process the input data\n    result = process_data(input_data[\"value\"])\n    \n    return {\"success\": True, \"result\": result}\n\n# Create a task with the direct handler\ntask = DirectHandlerTask(\n    task_id=\"custom_processor\",\n    name=\"Custom Data Processor\",\n    handler=my_custom_handler,\n    input_data={\"value\": \"sample_data\"}\n)\n\n# Add to workflow\nworkflow.add_task(task)\n```\n\n> **Note:** DirectHandlerTask now supports both older single-parameter handlers (just `input_data`) and the new two-parameter format (`task, input_data`). The two-parameter format is recommended as it provides access to the task context.\n\n### Services Container\n\nThe Services Container provides centralized management of framework dependencies and ensures consistent access to singleton services.\n\n```python\nfrom core.services import get_services\n\n# Get the services container\nservices = get_services()\n\n# Access the tool registry (singleton)\nregistry = services.tool_registry\n\n# Register a custom LLM interface\nfrom core.llm.interface import LLMInterface\nllm = LLMInterface(model=\"gpt-4\")\nservices.register_llm_interface(llm, name=\"gpt4_interface\")\n\n# Create an agent using services\nagent = Agent(\n    agent_id=\"my_agent\",\n    name=\"My Agent\",\n    # The registry will be automatically provided from the services container\n    # Use a specific named LLM interface\n    llm_interface=services.get_llm_interface(\"gpt4_interface\")\n)\n```\n\n## Examples\n\n- **Complex Conditional Workflow**: `examples/complex_conditional_workflow.py`\n- **Simple Conditional Workflow**: `examples/simple_conditional_workflow.py`\n- **Complex Workflow**: `examples/complex_workflow.py`\n- **Vector Store Example**: `examples/vector_store_example.py`\n\n## Overview\n\nThe AI Agent Framework is an open-source Python framework designed to simplify the development of agent-based applications. It features a robust and explicit system for dynamic workflow management, allowing agents to break down complex tasks, execute sub-tasks, evaluate results, and dynamically adjust their action plans. This improves agent reliability and adaptability.\n\n## Features\n\n- **Dynamic Workflow Management System (WMS):**\n  - Explicit task representation with state, dependencies, and tools.\n  - Definition of workflows (sequential, parallel, conditional).\n  - Monitored task execution with feedback integration for decision-making.\n  - Workflow state tracking.\n  - **Visualization of Workflow Execution:** Generate visual representations of workflows to aid in understanding and debugging.\n\n- **LLM Interface:**\n  - Compatibility with popular APIs (e.g., OpenAI).\n  - Simple request/response pattern with function/tool calling.\n\n- **Tool Interface:**\n  - Easy integration of custom tools.\n  - Example tools include a calculator for arithmetic operations.\n\n- **Web Search Tool:**\n  - Allows models to search the web for the latest information.\n  - Configured in the `tools` array of an API request.\n  - Supports user location and search context size customization.\n\n- **Vector Store Tools:**\n  - Create and manage OpenAI Vector Stores for efficient data storage and retrieval.\n  - Utilities for validating vector store IDs with basic and strict validation options.\n  - Integration with file uploads and text storage for comprehensive knowledge management.\n\n- **DirectHandlerTask Support:**\n  - Register and use handler functions directly in workflows without needing global tool registry.\n  - Improved task output and variable resolution for complex data structures.\n  - Support for both synchronous and asynchronous workflow engines.\n  - Enhanced debugging and error reporting for task execution.\n\n- **Observability:**\n  - Logging and tracing to support debugging and workflow analysis.\n\n## Dependencies\n\nThe framework has the following core dependencies:\n\n- **OpenAI API**: For LLM access and vector store operations\n- **HTTPX**: For direct API calls when the OpenAI client doesn't support certain operations\n- **Pydantic**: For data validation and settings management\n- **Graphviz** (optional): For workflow visualization\n- **python-dotenv**: For environment variable management\n\n## Installation\n\n1. **Clone the Repository:**\n\n   ```bash\n   git clone <repository-url>\n   cd <repository-directory>\n   ```\n\n2. **Set Up Virtual Environment:**\n\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n   ```\n\n3. **Install Dependencies:**\n\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. **Install Development Dependencies (optional):**\n\n   ```bash\n   pip install -r requirements-dev.txt\n   ```\n\n5. **Set Up Environment Variables:**\n   - Create a `.env` file in the root directory.\n   - Add your OpenAI API key:\n\n     ```\n     OPENAI_API_KEY=your_openai_api_key\n     ```\n\n## Usage\n\n1. **Run the Example Workflow:**\n\n   ```bash\n   python examples/simple_workflow.py\n   ```\n\n2. **Define Your Own Workflow:**\n   - Create tasks using the `Task` class.\n   - Define workflows using the `Workflow` class.\n   - Use the `Agent` class to load and run workflows.\n\n3. **Integrate Web Search Tool:**\n   - Use the Web Search tool to enhance your agent's ability to access real-time information.\n   - Configure the tool in the `tools` array of your API requests.\n\n4. **Visualize Workflow Execution:**\n   - Use the `visualize_workflow` function to generate visual representations of your workflows.\n\n## Development\n\n### Code Style and Linting\n\nThis project uses several tools to maintain code quality:\n\n- **Black**: For code formatting (line length: 120 characters)\n- **isort**: For sorting imports (configured to work with Black)\n- **Flake8**: For linting and style checking\n\nThe project is configured with appropriate settings for each tool:\n- Line length is set to 120 characters for all tools\n- Specific file patterns have customized linting rules\n- Common errors are selectively ignored in example and test files\n\n#### Setup Pre-commit Hooks\n\n```bash\npre-commit install\n```\n\n#### Manual Linting\n\nYou can use the Makefile or the lint script for linting:\n\n```bash\n# Using Make\nmake format    # Run isort and black\nmake lint      # Run flake8\nmake lint-all  # Run both formatting and linting\n\n# Using the lint script\n./lint.sh\n```\n\n#### Running Tests\n\n```bash\nmake test      # Run all tests with pytest\nPYTHONPATH=.  python -m unittest tests/test_file_name.py  # Run a specific test file\n```\n\n#### Linting Configuration\n\nThe project includes the following configuration files:\n- `setup.cfg`: Contains settings for flake8 and isort\n  - Configures what errors to ignore globally and per file\n  - Ignores common docstring-related errors (D100-D107, D200, D205, D400, D401)\n- `pyproject.toml`: Contains settings for black\n- `.pre-commit-config.yaml`: Configures pre-commit hooks for automatic linting\n  - Includes flake8-docstrings as an additional dependency for pre-commit checks\n\n**Note on docstring checking**: When running `flake8` directly, it uses the configuration in `setup.cfg`. \nHowever, the pre-commit hook includes additional docstring checking through flake8-docstrings. \nThe setup is configured to ignore most docstring formatting errors to maintain a balance between \ncode quality and development speed.\n\n## Key Concepts\n\n- **Task:** Represents a unit of work with attributes like `id`, `name`, `status`, `input_data`, `output_data`, `is_llm_task`, and `tool_name`.\n- **Workflow:** A collection of tasks with a defined execution order and logic.\n- **Agent:** Manages workflows, LLM interfaces, and tool registries, providing methods to run workflows and get results.\n- **Tool Interface:** Allows for the registration and execution of custom tools, including the Web Search tool.\n\n## Development Plan\n\nThe project is structured into phases, with the initial version focusing on core functionality and basic examples. Future enhancements may include more complex multi-agent orchestration and advanced state persistence.\n\n## Contributing\n\nContributions are welcome! Please fork the repository and submit a pull request with your changes. Ensure that your code adheres to the project's coding standards and includes appropriate tests.\n\nBefore submitting a pull request:\n1. Run the linting tools to ensure code quality\n2. Make sure all tests pass\n3. Update documentation if necessary\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n## Contact\n\nFor questions or feedback, please contact Enrique Meza C: emezac at [gmail.com]\n\n## Documentation\n\nThe project includes comprehensive documentation:\n\n- **Usage Guides**: Examples and tutorials on how to use the framework effectively.\n- **API Reference**: Documentation of all public classes, methods, and interfaces.\n- **Tool Documentation**: Detailed information on available tools and their usage.\n- **Validation Utilities**: Guidelines for validating inputs and outputs, including [Vector Store ID Validation](docs/vector_store_id_validation.md).\n- **OpenAI Integration**: Information on working with OpenAI APIs, including:\n  - [File Purpose Parameters](docs/openai_file_purpose_parameters.md)\n  - [API Parameter Changes](docs/openai_api_parameter_changes.md) (including Beta API header format)\n- **Fixes and Enhancements**: Documentation of known issues and their solutions in the `fixes` directory.\n",
        "temperature": 0.0,
        "max_tokens": 100
      },
      "output": {
        "success": true,
        "content": "Mock response for prompt: Please extract all entities (p...",
        "status": "success",
        "timestamp": "2025-04-12T00:45:01.475039"
      },
      "success": true,
      "timestamp": "2025-04-12T00:45:01.475042"
    },
    {
      "tool_name": "openai",
      "inputs": {
        "prompt": "Please provide a concise summary of the following document:\n\n# AI Agent Framework\n\nThe AI Agent Framework is designed to enhance the capabilities of AI agents, allowing them to perform tasks such as web searching, file reading, file uploading, and markdown file creation. Recent updates include support for conditional workflows, enhancements to the web search tool, and the ability to visualize workflow execution.\n\n## Features\n\n- **Conditional Workflows**: Enable dynamic task execution based on specific conditions or task outcomes, enhancing flexibility and adaptability.\n- **Web Search Tool**: Updated to use the latest model version with improved performance and reliability.\n- **Visualization of Workflow Execution**: Generate visual representations of workflows to aid in understanding and debugging.\n- **Vector Store ID Validation**: Robust utilities for validating OpenAI Vector Store IDs with different validation levels.\n- **Error Propagation Between Tasks**: Comprehensive error tracking and propagation system allowing downstream tasks to access and handle errors from upstream tasks.\n- **DirectHandlerTask Support:**\n  - Register and use handler functions directly in workflows without needing global tool registry.\n  - Improved task output and variable resolution for complex data structures.\n  - Support for both synchronous and asynchronous workflow engines.\n  - Enhanced debugging and error reporting for task execution.\n- **Observability:**\n  - Logging and tracing to support debugging and workflow analysis.\n- **Services Container:**\n  - Centralized dependency management for framework-wide singletons\n  - Type-safe access to shared services like ToolRegistry and LLMInterface\n  - Proper dependency injection support throughout the framework\n  - Simplified management of service lifecycle and configuration\n\n## Usage\n\n### Conditional Workflows\n\nConditional workflows allow for dynamic task execution based on specific conditions or task outcomes. This feature enhances the framework's flexibility and adaptability.\n\n#### Usage Example\n\n```python\nworkflow = [\n    {\n        \"id\": \"task_1\",\n        \"tool\": \"llm_generate_idea\",\n        \"condition\": None\n    },\n    {\n        \"id\": \"task_2a\",\n        \"tool\": \"draft_email\",\n        \"condition\": {\n            \"depends_on\": \"task_1\",\n            \"if_result\": \"good\"\n        }\n    },\n    ...\n]\n```\n\n### Web Search Tool\n\nThe web search tool has been updated to use the latest model version and includes a timeout setting for improved performance.\n\n#### Usage Example\n\n```python\ninput_data = {\n    \"query\": \"What was a positive news story from today?\",\n    \"context_size\": \"medium\"\n}\nresult = registry.execute_tool(\"web_search\", input_data)\n```\n\n### Visualization of Workflow Execution\n\nThe framework supports generating visual representations of workflows using Graphviz, which helps in understanding and debugging complex workflows.\n\n#### Usage Example\n\n```python\nfrom core.utils.visualizer import visualize_workflow\n\n# Assuming 'workflow' is an instance of the Workflow class\nvisualize_workflow(workflow, filename=\"workflow_graph\", format='pdf', view=True)\n```\n\n### Vector Store ID Validation\n\nThe framework provides utilities for validating OpenAI Vector Store IDs, ensuring consistent validation across all tools and workflows.\n\n#### Usage Example\n\n```python\nfrom tools.openai_vs.utils.vs_id_validator import (\n    is_valid_vector_store_id, \n    is_strict_valid_vector_store_id,\n    validate_vector_store_id,\n    assert_valid_vector_store_id\n)\n\n# Simple boolean validation\nif is_valid_vector_store_id(vector_store_id):\n    # Perform operation\n\n# Strict validation with regex pattern\nif is_strict_valid_vector_store_id(vector_store_id):\n    # Perform operation requiring strict format\n\n# Get validation status and error message\nis_valid, error_message = validate_vector_store_id(vector_store_id, strict=True)\nif not is_valid:\n    print(f\"Invalid vector store ID: {error_message}\")\n\n# Assert valid ID (raises ValueError if invalid)\ntry:\n    assert_valid_vector_store_id(vector_store_id, strict=False)\n    # Continue with operation\nexcept ValueError as e:\n    print(f\"Validation error: {e}\")\n```\n\nFor more details on vector store ID validation, see the [Vector Store ID Validation documentation](docs/vector_store_id_validation.md).\n\n### Error Propagation Between Tasks\n\nThe framework now supports robust error propagation between tasks, allowing workflows to implement sophisticated error handling and recovery strategies.\n\n#### Usage Example\n\n```python\n# Define a task that might fail\ndata_processing_task = Task(\n    task_id=\"process_data\",\n    name=\"Process Data\",\n    tool_name=\"data_processor\",\n    input_data={\"data_source\": \"customer_data.csv\"},\n    next_task_id_on_success=\"analyze_data\",  # Normal path on success\n    next_task_id_on_failure=\"handle_error\"   # Error handling path on failure\n)\n\n# Define an error handler task\nerror_handler_task = Task(\n    task_id=\"handle_error\",\n    name=\"Error Handler\",\n    tool_name=\"error_recovery_tool\",\n    input_data={\n        # Access error information from the failed task\n        \"error_message\": \"${error.process_data}\",\n        \"error_code\": \"${error.process_data.error_code}\",\n        \"error_details\": \"${error.process_data.error_details}\",\n        \"original_input\": \"customer_data.csv\"\n    },\n    next_task_id_on_success=\"retry_processing\",  # Try again if recovery succeeded\n    next_task_id_on_failure=\"report_failure\"      # Report failure if recovery fails\n)\n\n# Add tasks to the workflow\nworkflow.add_task(data_processing_task)\nworkflow.add_task(error_handler_task)\n```\n\nThe error propagation system automatically tracks errors across tasks and provides:\n- Detailed error information with standardized format\n- Error references in task inputs using the `${error.task_id}` syntax\n- Error propagation chains to track error origins\n- Workflow-level error summaries for monitoring and analysis\n\nFor more details, see the [Error Propagation documentation](docs/error_propagation.md).\n\n### DirectHandlerTask Support\n\n```python\nfrom core.task import DirectHandlerTask\n\n# Define a handler function with the new two-parameter format\ndef my_custom_handler(task, input_data):\n    # Access task properties if needed\n    print(f\"Executing task: {task.name} (ID: {task.id})\")\n    \n    # Process the input data\n    result = process_data(input_data[\"value\"])\n    \n    return {\"success\": True, \"result\": result}\n\n# Create a task with the direct handler\ntask = DirectHandlerTask(\n    task_id=\"custom_processor\",\n    name=\"Custom Data Processor\",\n    handler=my_custom_handler,\n    input_data={\"value\": \"sample_data\"}\n)\n\n# Add to workflow\nworkflow.add_task(task)\n```\n\n> **Note:** DirectHandlerTask now supports both older single-parameter handlers (just `input_data`) and the new two-parameter format (`task, input_data`). The two-parameter format is recommended as it provides access to the task context.\n\n### Services Container\n\nThe Services Container provides centralized management of framework dependencies and ensures consistent access to singleton services.\n\n```python\nfrom core.services import get_services\n\n# Get the services container\nservices = get_services()\n\n# Access the tool registry (singleton)\nregistry = services.tool_registry\n\n# Register a custom LLM interface\nfrom core.llm.interface import LLMInterface\nllm = LLMInterface(model=\"gpt-4\")\nservices.register_llm_interface(llm, name=\"gpt4_interface\")\n\n# Create an agent using services\nagent = Agent(\n    agent_id=\"my_agent\",\n    name=\"My Agent\",\n    # The registry will be automatically provided from the services container\n    # Use a specific named LLM interface\n    llm_interface=services.get_llm_interface(\"gpt4_interface\")\n)\n```\n\n## Examples\n\n- **Complex Conditional Workflow**: `examples/complex_conditional_workflow.py`\n- **Simple Conditional Workflow**: `examples/simple_conditional_workflow.py`\n- **Complex Workflow**: `examples/complex_workflow.py`\n- **Vector Store Example**: `examples/vector_store_example.py`\n\n## Overview\n\nThe AI Agent Framework is an open-source Python framework designed to simplify the development of agent-based applications. It features a robust and explicit system for dynamic workflow management, allowing agents to break down complex tasks, execute sub-tasks, evaluate results, and dynamically adjust their action plans. This improves agent reliability and adaptability.\n\n## Features\n\n- **Dynamic Workflow Management System (WMS):**\n  - Explicit task representation with state, dependencies, and tools.\n  - Definition of workflows (sequential, parallel, conditional).\n  - Monitored task execution with feedback integration for decision-making.\n  - Workflow state tracking.\n  - **Visualization of Workflow Execution:** Generate visual representations of workflows to aid in understanding and debugging.\n\n- **LLM Interface:**\n  - Compatibility with popular APIs (e.g., OpenAI).\n  - Simple request/response pattern with function/tool calling.\n\n- **Tool Interface:**\n  - Easy integration of custom tools.\n  - Example tools include a calculator for arithmetic operations.\n\n- **Web Search Tool:**\n  - Allows models to search the web for the latest information.\n  - Configured in the `tools` array of an API request.\n  - Supports user location and search context size customization.\n\n- **Vector Store Tools:**\n  - Create and manage OpenAI Vector Stores for efficient data storage and retrieval.\n  - Utilities for validating vector store IDs with basic and strict validation options.\n  - Integration with file uploads and text storage for comprehensive knowledge management.\n\n- **DirectHandlerTask Support:**\n  - Register and use handler functions directly in workflows without needing global tool registry.\n  - Improved task output and variable resolution for complex data structures.\n  - Support for both synchronous and asynchronous workflow engines.\n  - Enhanced debugging and error reporting for task execution.\n\n- **Observability:**\n  - Logging and tracing to support debugging and workflow analysis.\n\n## Dependencies\n\nThe framework has the following core dependencies:\n\n- **OpenAI API**: For LLM access and vector store operations\n- **HTTPX**: For direct API calls when the OpenAI client doesn't support certain operations\n- **Pydantic**: For data validation and settings management\n- **Graphviz** (optional): For workflow visualization\n- **python-dotenv**: For environment variable management\n\n## Installation\n\n1. **Clone the Repository:**\n\n   ```bash\n   git clone <repository-url>\n   cd <repository-directory>\n   ```\n\n2. **Set Up Virtual Environment:**\n\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n   ```\n\n3. **Install Dependencies:**\n\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. **Install Development Dependencies (optional):**\n\n   ```bash\n   pip install -r requirements-dev.txt\n   ```\n\n5. **Set Up Environment Variables:**\n   - Create a `.env` file in the root directory.\n   - Add your OpenAI API key:\n\n     ```\n     OPENAI_API_KEY=your_openai_api_key\n     ```\n\n## Usage\n\n1. **Run the Example Workflow:**\n\n   ```bash\n   python examples/simple_workflow.py\n   ```\n\n2. **Define Your Own Workflow:**\n   - Create tasks using the `Task` class.\n   - Define workflows using the `Workflow` class.\n   - Use the `Agent` class to load and run workflows.\n\n3. **Integrate Web Search Tool:**\n   - Use the Web Search tool to enhance your agent's ability to access real-time information.\n   - Configure the tool in the `tools` array of your API requests.\n\n4. **Visualize Workflow Execution:**\n   - Use the `visualize_workflow` function to generate visual representations of your workflows.\n\n## Development\n\n### Code Style and Linting\n\nThis project uses several tools to maintain code quality:\n\n- **Black**: For code formatting (line length: 120 characters)\n- **isort**: For sorting imports (configured to work with Black)\n- **Flake8**: For linting and style checking\n\nThe project is configured with appropriate settings for each tool:\n- Line length is set to 120 characters for all tools\n- Specific file patterns have customized linting rules\n- Common errors are selectively ignored in example and test files\n\n#### Setup Pre-commit Hooks\n\n```bash\npre-commit install\n```\n\n#### Manual Linting\n\nYou can use the Makefile or the lint script for linting:\n\n```bash\n# Using Make\nmake format    # Run isort and black\nmake lint      # Run flake8\nmake lint-all  # Run both formatting and linting\n\n# Using the lint script\n./lint.sh\n```\n\n#### Running Tests\n\n```bash\nmake test      # Run all tests with pytest\nPYTHONPATH=.  python -m unittest tests/test_file_name.py  # Run a specific test file\n```\n\n#### Linting Configuration\n\nThe project includes the following configuration files:\n- `setup.cfg`: Contains settings for flake8 and isort\n  - Configures what errors to ignore globally and per file\n  - Ignores common docstring-related errors (D100-D107, D200, D205, D400, D401)\n- `pyproject.toml`: Contains settings for black\n- `.pre-commit-config.yaml`: Configures pre-commit hooks for automatic linting\n  - Includes flake8-docstrings as an additional dependency for pre-commit checks\n\n**Note on docstring checking**: When running `flake8` directly, it uses the configuration in `setup.cfg`. \nHowever, the pre-commit hook includes additional docstring checking through flake8-docstrings. \nThe setup is configured to ignore most docstring formatting errors to maintain a balance between \ncode quality and development speed.\n\n## Key Concepts\n\n- **Task:** Represents a unit of work with attributes like `id`, `name`, `status`, `input_data`, `output_data`, `is_llm_task`, and `tool_name`.\n- **Workflow:** A collection of tasks with a defined execution order and logic.\n- **Agent:** Manages workflows, LLM interfaces, and tool registries, providing methods to run workflows and get results.\n- **Tool Interface:** Allows for the registration and execution of custom tools, including the Web Search tool.\n\n## Development Plan\n\nThe project is structured into phases, with the initial version focusing on core functionality and basic examples. Future enhancements may include more complex multi-agent orchestration and advanced state persistence.\n\n## Contributing\n\nContributions are welcome! Please fork the repository and submit a pull request with your changes. Ensure that your code adheres to the project's coding standards and includes appropriate tests.\n\nBefore submitting a pull request:\n1. Run the linting tools to ensure code quality\n2. Make sure all tests pass\n3. Update documentation if necessary\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n## Contact\n\nFor questions or feedback, please contact Enrique Meza C: emezac at [gmail.com]\n\n## Documentation\n\nThe project includes comprehensive documentation:\n\n- **Usage Guides**: Examples and tutorials on how to use the framework effectively.\n- **API Reference**: Documentation of all public classes, methods, and interfaces.\n- **Tool Documentation**: Detailed information on available tools and their usage.\n- **Validation Utilities**: Guidelines for validating inputs and outputs, including [Vector Store ID Validation](docs/vector_store_id_validation.md).\n- **OpenAI Integration**: Information on working with OpenAI APIs, including:\n  - [File Purpose Parameters](docs/openai_file_purpose_parameters.md)\n  - [API Parameter Changes](docs/openai_api_parameter_changes.md) (including Beta API header format)\n- **Fixes and Enhancements**: Documentation of known issues and their solutions in the `fixes` directory.\n",
        "temperature": 0.3,
        "max_tokens": 200
      },
      "output": {
        "success": true,
        "content": "Mock response for prompt: Please provide a concise summa...",
        "status": "success",
        "timestamp": "2025-04-12T00:45:01.475062"
      },
      "success": true,
      "timestamp": "2025-04-12T00:45:01.475064"
    }
  ]
}