#!/usr/bin/env python
"""
Test script for validating and converting a research plan.
This script simulates how the validate_plan_handler and convert_plan_to_tasks_handler
functions process a plan generated by an LLM.
"""  # noqa: D202

import json
import sys
import os
import re

# Add the root directory to path for imports
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Try to import the handler functions
try:
    from examples.economic_impact_researcher import validate_plan_handler
    from examples.economic_impact_researcher import convert_plan_to_tasks_handler
    from examples.economic_impact_researcher import PLAN_SCHEMA
except ImportError as e:
    print(f"Error importing handler functions: {e}")
    sys.exit(1)

def simulate_fixed_validate_plan_handler(raw_llm_output, debug=True):
    """Simulate the fixed validate_plan_handler function with a given input."""
    if debug:
        print("\n\n=== STARTING VALIDATE PLAN HANDLER SIMULATION ===")
        print(f"Input raw_llm_output type: {type(raw_llm_output)}")
        print(f"Input raw_llm_output length: {len(str(raw_llm_output))}")
        print(f"Input raw_llm_output preview: {str(raw_llm_output)[:200]}...")
    
    # Preprocess raw_llm_output to extract just the steps array if it's a full plan JSON
    try:
        # Try to parse the raw output to see if it contains a methodology.steps field
        if isinstance(raw_llm_output, str):
            try:
                # Direct JSON parsing
                plan_obj = json.loads(raw_llm_output)
                if isinstance(plan_obj, dict) and 'methodology' in plan_obj and 'steps' in plan_obj['methodology']:
                    # Extract just the steps array for validation
                    steps_array = plan_obj['methodology']['steps']
                    # Convert steps to format expected by PLAN_SCHEMA
                    converted_steps = []
                    for step in steps_array:
                        if isinstance(step, dict):
                            # Map fields from sample format to expected format
                            converted_step = {
                                "step_id": step.get("id", ""),
                                "description": step.get("description", ""),
                                "type": "tool",  # Default to tool
                                "name": "web_search",  # Default to web_search
                                "inputs": {"query": step.get("description", "")},
                                "depends_on": step.get("dependencies", []),
                                "outputs": [f"{step.get('id', '')}_results"]
                            }
                            converted_steps.append(converted_step)
                    if converted_steps:
                        # Use the converted steps array
                        raw_llm_output = json.dumps(converted_steps)
                        print(f"Extracted and converted {len(converted_steps)} steps from methodology.steps")
            except json.JSONDecodeError:
                # Try to extract from markdown code blocks
                json_block_pattern = r"```(?:json)?\s*([\s\S]*?)\s*```"
                matches = re.findall(json_block_pattern, raw_llm_output)
                if matches:
                    try:
                        plan_obj = json.loads(matches[0])
                        if isinstance(plan_obj, dict) and 'methodology' in plan_obj and 'steps' in plan_obj['methodology']:
                            # Extract just the steps array for validation
                            steps_array = plan_obj['methodology']['steps']
                            # Convert steps to format expected by PLAN_SCHEMA
                            converted_steps = []
                            for step in steps_array:
                                if isinstance(step, dict):
                                    # Map fields from sample format to expected format
                                    converted_step = {
                                        "step_id": step.get("id", ""),
                                        "description": step.get("description", ""),
                                        "type": "tool",  # Default to tool
                                        "name": "web_search",  # Default to web_search
                                        "inputs": {"query": step.get("description", "")},
                                        "depends_on": step.get("dependencies", []),
                                        "outputs": [f"{step.get('id', '')}_results"]
                                    }
                                    converted_steps.append(converted_step)
                            if converted_steps:
                                # Use the converted steps array
                                raw_llm_output = json.dumps(converted_steps)
                                print(f"Extracted and converted {len(converted_steps)} steps from markdown code block")
                    except json.JSONDecodeError:
                        pass
    except Exception as e:
        print(f"Error during preprocessing: {e}")
    
    task_input_data = {"raw_llm_output": raw_llm_output}
    task_context = {}
    
    # Call the handler
    result = validate_plan_handler(task_input_data, task_context)
    
    if debug:
        print("\nValidation Result:")
        print(f"Success: {result.get('success')}")
        print(f"Status: {result.get('status')}")
        if result.get('success'):
            print("Validated plan preview:")
            validated_plan = result.get('result', {}).get('validated_plan')
            if isinstance(validated_plan, str):
                try:
                    parsed = json.loads(validated_plan)
                    print(json.dumps(parsed, indent=2)[:300])
                except:
                    print(f"Could not parse plan as JSON. Preview: {validated_plan[:200]}")
            else:
                print(json.dumps(validated_plan, indent=2)[:300])
        else:
            print(f"Error: {result.get('result', {}).get('error')}")
        
        print("=== END VALIDATE PLAN HANDLER SIMULATION ===\n")
    
    return result

def simulate_convert_plan_to_tasks(validated_plan_json, debug=True):
    """Simulate the convert_plan_to_tasks_handler function with a given input."""
    if debug:
        print("\n\n=== STARTING CONVERT PLAN TO TASKS SIMULATION ===")
        if isinstance(validated_plan_json, str):
            print(f"Input validated_plan_json type: string of length {len(validated_plan_json)}")
            print(f"Input validated_plan_json preview: {validated_plan_json[:200]}...")
        else:
            print(f"Input validated_plan_json type: {type(validated_plan_json)}")
            print(f"Input validated_plan_json preview: {json.dumps(validated_plan_json, indent=2)[:200]}...")
    
    task_input_data = {"validated_plan_json": validated_plan_json}
    task_context = {}
    
    # Call the handler
    result = convert_plan_to_tasks_handler(task_input_data, task_context)
    
    if debug:
        print("\nConversion Result:")
        print(f"Success: {result.get('success')}")
        print(f"Status: {result.get('status')}")
        if result.get('success'):
            print("Dynamic tasks preview:")
            tasks_json = result.get('result', {}).get('dynamic_tasks_json')
            print(json.dumps(tasks_json, indent=2)[:300])
        else:
            print(f"Error: {result.get('result', {}).get('error')}")
        
        print("=== END CONVERT PLAN TO TASKS SIMULATION ===\n")
    
    return result

def main():
    """Main function to run the validation and conversion test."""
    print("=== TESTING RESEARCH PLAN VALIDATION AND CONVERSION ===")
    
    # Define the sample steps directly in the expected format for PLAN_SCHEMA
    sample_plan_json = [
        {
            "step_id": "us_sectors",
            "description": "Research which US sectors were most impacted by tariffs",
            "type": "tool",
            "name": "web_search",
            "inputs": {"query": "US sectors impacted by Trump tariffs"},
            "outputs": ["us_sectors_results"],
            "depends_on": ["start"]
        },
        {
            "step_id": "international_response",
            "description": "Research international responses to US tariffs",
            "type": "tool",
            "name": "web_search",
            "inputs": {"query": "international responses to Trump tariffs"},
            "outputs": ["international_response_results"],
            "depends_on": ["start"]
        },
        {
            "step_id": "consumer_prices",
            "description": "Research impact on US consumer prices",
            "type": "tool",
            "name": "web_search",
            "inputs": {"query": "impact on US consumer prices from tariffs"},
            "outputs": ["consumer_prices_results"],
            "depends_on": ["us_sectors"]
        },
        {
            "step_id": "employment_effects",
            "description": "Research effects on employment in affected sectors",
            "type": "tool",
            "name": "web_search",
            "inputs": {"query": "employment effects of Trump tariffs"},
            "outputs": ["employment_effects_results"],
            "depends_on": ["us_sectors"]
        },
        {
            "step_id": "trade_deficit",
            "description": "Research changes in US trade deficit",
            "type": "tool",
            "name": "web_search",
            "inputs": {"query": "changes in US trade deficit from Trump tariffs"},
            "outputs": ["trade_deficit_results"],
            "depends_on": ["international_response"]
        },
        {
            "step_id": "analyze_results",
            "description": "Analyze research findings",
            "type": "handler",
            "name": "analyze_text",
            "inputs": {
                "text_to_analyze": "${us_sectors.result} ${international_response.result} ${consumer_prices.result} ${employment_effects.result} ${trade_deficit.result}",
                "analysis_type": "summarize"
            },
            "outputs": ["analysis_results"],
            "depends_on": ["us_sectors", "international_response", "consumer_prices", "employment_effects", "trade_deficit"]
        }
    ]
    
    # Format it as if it came from an LLM (in different possible formats)
    # 1. As a JSON string directly
    raw_output_json = json.dumps(sample_plan_json, indent=2)
    
    # 2. As a JSON embedded in markdown code blocks
    raw_output_markdown = f"""
Here's the research plan I've developed:

```json
{json.dumps(sample_plan_json, indent=2)}
```

This plan should provide a comprehensive analysis of the economic impacts.
    """
    
    # 3. As JSON embedded in text
    raw_output_text = f"""
I've analyzed your request and created a research plan for studying the economic impact of Trump's tariffs.

The research plan is as follows:

{json.dumps(sample_plan_json, indent=2)}

Let me know if you'd like any adjustments to this plan.
    """
    
    # Test all three formats
    print("\n=== TESTING JSON FORMAT ===")
    validation_result_1 = simulate_fixed_validate_plan_handler(raw_output_json)
    if validation_result_1.get('success'):
        convert_result_1 = simulate_convert_plan_to_tasks(
            validation_result_1.get('result', {}).get('validated_plan')
        )
    
    print("\n=== TESTING MARKDOWN FORMAT ===")
    validation_result_2 = simulate_fixed_validate_plan_handler(raw_output_markdown)
    if validation_result_2.get('success'):
        convert_result_2 = simulate_convert_plan_to_tasks(
            validation_result_2.get('result', {}).get('validated_plan')
        )
    
    print("\n=== TESTING TEXT FORMAT ===")
    validation_result_3 = simulate_fixed_validate_plan_handler(raw_output_text)
    if validation_result_3.get('success'):
        convert_result_3 = simulate_convert_plan_to_tasks(
            validation_result_3.get('result', {}).get('validated_plan')
        )
    
    print("\n=== RESULTS SUMMARY ===")
    print(f"JSON Format: Validation {'✅' if validation_result_1.get('success') else '❌'}, Conversion {'✅' if validation_result_1.get('success') and convert_result_1.get('success') else '❌'}")
    print(f"Markdown Format: Validation {'✅' if validation_result_2.get('success') else '❌'}, Conversion {'✅' if validation_result_2.get('success') and convert_result_2.get('success') else '❌'}")
    print(f"Text Format: Validation {'✅' if validation_result_3.get('success') else '❌'}, Conversion {'✅' if validation_result_3.get('success') and convert_result_3.get('success') else '❌'}")
    
if __name__ == "__main__":
    main() 